{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sin, cos, pi\n",
    "\n",
    "from gym import core, spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import csv\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import GenOnlyModel\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "import gc\n",
    "import keras.backend\n",
    "\n",
    "import objgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_33\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 15)        60          input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_43 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_187 (Dense)               (None, 1, 15)        240         embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 15)           30          input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 1, 15)        60          dense_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 15)           60          dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, 1, 15)        240         batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, 15)           240         batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 1, 15)        60          dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 10)           20          input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 15)           60          dense_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 1, 15)        240         batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_45 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 10)           40          dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_194 (Dense)               (None, 15)           240         batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 1, 15)        60          dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_198 (Dense)               (None, 50)           2550        input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 10)           110         batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 15)           60          dense_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_196 (Dense)               (None, 10)           20          input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_60 (Reshape)            (None, 15, 1)        0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_64 (Reshape)            (None, 50, 1)        0           dense_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 10)           40          dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_195 (Dense)               (None, 15)           240         batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 10)           40          dense_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 65, 1)        0           reshape_60[0][0]                 \n",
      "                                                                 reshape_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_61 (Reshape)            (None, 10, 1)        0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 15)           60          dense_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_197 (Dense)               (None, 10)           110         batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 75, 1)        0           concatenate_24[0][0]             \n",
      "                                                                 reshape_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_62 (Reshape)            (None, 15, 1)        0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 10)           40          dense_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 90, 1)        0           concatenate_25[0][0]             \n",
      "                                                                 reshape_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_63 (Reshape)            (None, 10, 1)        0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 100, 1)       0           concatenate_26[0][0]             \n",
      "                                                                 reshape_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 100)          0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_199 (Dense)               (None, 100)          10100       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 100)          0           dense_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 100)          400         leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_200 (Dense)               (None, 100)          10100       batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 100)          0           flatten_11[0][0]                 \n",
      "                                                                 dense_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 100)          0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 100)          400         leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_201 (Dense)               (None, 200)          20200       batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_65 (Reshape)            (None, 100, 1)       0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 200)          0           dense_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_12 (UpSampling1D) (None, 200, 1)       0           reshape_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 200)          800         leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_66 (Reshape)            (None, 1, 200)       0           up_sampling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, 200)          40200       batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 1, 200)       0           reshape_66[0][0]                 \n",
      "                                                                 dense_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 1, 200)       0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 1, 200)       800         leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_203 (Dense)               (None, 1, 200)       40200       batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 1, 200)       0           dense_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 1, 200)       800         leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, 1, 200)       40200       batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 1, 200)       0           batch_normalization_164[0][0]    \n",
      "                                                                 dense_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 1, 200)       0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 1, 200)       800         leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_205 (Dense)               (None, 1, 200)       40200       batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 1, 200)       0           dense_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 1, 200)       800         leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_206 (Dense)               (None, 1, 200)       40200       batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 1, 200)       0           batch_normalization_166[0][0]    \n",
      "                                                                 dense_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 1, 200)       0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 1, 200)       800         leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_207 (Dense)               (None, 1, 400)       80400       batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_67 (Reshape)            (None, 200, 1)       0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 1, 400)       0           dense_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_13 (UpSampling1D) (None, 400, 1)       0           reshape_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 1, 400)       1600        leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_68 (Reshape)            (None, 1, 400)       0           up_sampling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_208 (Dense)               (None, 1, 400)       160400      batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 1, 400)       0           reshape_68[0][0]                 \n",
      "                                                                 dense_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 1, 400)       0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 1, 400)       1600        leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, 1, 400)       160400      batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 1, 400)       0           dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 1, 400)       1600        leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 1, 400)       160400      batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 1, 400)       0           batch_normalization_170[0][0]    \n",
      "                                                                 dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)      (None, 1, 400)       0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 1, 400)       1600        leaky_re_lu_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 1, 400)       160400      batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)      (None, 1, 400)       0           dense_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 1, 400)       1600        leaky_re_lu_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 1, 400)       160400      batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 1, 400)       0           batch_normalization_172[0][0]    \n",
      "                                                                 dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)      (None, 1, 400)       0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 1, 400)       1600        leaky_re_lu_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 1, 400)       160400      batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_69 (Reshape)            (None, 400, 1)       0           dense_213[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,304,320\n",
      "Trainable params: 1,296,430\n",
      "Non-trainable params: 7,890\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  0 score -4.0: steps 9.0: average_score -4.0:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -7.0:  steps 9.0: \n",
      "episode:  1 score -7.0: steps 9.0: average_score -5.5:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 9.0: \n",
      "episode:  2 score -7.0: steps 9.0: average_score -6.0:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 9.0: \n",
      "episode:  3 score -5.0: steps 9.0: average_score -5.8:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  4 score -4.0: steps 9.0: average_score -5.4:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  5 score -4.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 9.0: \n",
      "episode:  6 score -7.0: steps 9.0: average_score -5.4:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  7 score -4.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  8 score -4.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 9.0: \n",
      "episode:  9 score -4.0: steps 9.0: average_score -5.0:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  10 score -4.0: steps 9.0: average_score -4.9:\n",
      "0\n",
      "action:  R score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 9.0: \n",
      "episode:  11 score -3.0: steps 9.0: average_score -4.8:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -7.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -8.0:  steps 9.0: \n",
      "episode:  12 score -8.0: steps 9.0: average_score -5.0:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 9.0: \n",
      "episode:  13 score -5.0: steps 9.0: average_score -5.0:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 1.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 2.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 9.0: \n",
      "episode:  14 score -4.0: steps 9.0: average_score -4.9:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 9.0: \n",
      "episode:  15 score -4.0: steps 9.0: average_score -4.9:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 9.0: \n",
      "episode:  16 score -6.0: steps 9.0: average_score -4.9:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 1.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 2.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 3.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 9.0: \n",
      "episode:  17 score -3.0: steps 9.0: average_score -4.8:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 7.0: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "action:  U score -5.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 9.0: \n",
      "episode:  18 score -5.0: steps 9.0: average_score -4.8:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  19 score -4.0: steps 9.0: average_score -4.8:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  20 score -4.0: steps 9.0: average_score -4.8:\n",
      "0\n",
      "action:  R score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 9.0: \n",
      "episode:  21 score -5.0: steps 9.0: average_score -4.8:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 9.0: \n",
      "episode:  22 score -4.0: steps 9.0: average_score -4.7:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 9.0: \n",
      "episode:  23 score -5.0: steps 9.0: average_score -4.8:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 9.0: \n",
      "episode:  24 score -6.0: steps 9.0: average_score -4.8:\n",
      "0\n",
      "action:  R score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 9.0: \n",
      "episode:  25 score -6.0: steps 9.0: average_score -4.8:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -7.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -7.0:  steps 9.0: \n",
      "episode:  26 score -7.0: steps 9.0: average_score -4.9:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  27 score -4.0: steps 9.0: average_score -4.9:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -6.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -7.0:  steps 9.0: \n",
      "episode:  28 score -7.0: steps 9.0: average_score -5.0:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -7.0:  steps 9.0: \n",
      "episode:  29 score -7.0: steps 9.0: average_score -5.0:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 9.0: \n",
      "episode:  30 score -7.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  31 score -4.0: steps 9.0: average_score -5.1:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 9.0: \n",
      "episode:  32 score -5.0: steps 9.0: average_score -5.1:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 1.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 9.0: \n",
      "episode:  33 score -5.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -7.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -7.0:  steps 9.0: \n",
      "episode:  34 score -7.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 9.0: \n",
      "episode:  35 score -4.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  36 score -4.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 3.0: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "action:  R score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  37 score -4.0: steps 9.0: average_score -5.0:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 9.0: \n",
      "episode:  38 score -6.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -7.0:  steps 9.0: \n",
      "episode:  39 score -7.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -7.0:  steps 9.0: \n",
      "episode:  40 score -7.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -5.0:  steps 6.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -6.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -6.0:  steps 9.0: \n",
      "episode:  41 score -6.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 9.0: \n",
      "episode:  42 score -5.0: steps 9.0: average_score -5.2:\n",
      "0\n",
      "action:  R score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 9.0: \n",
      "episode:  43 score -5.0: steps 9.0: average_score -5.2:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 1.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -6.0:  steps 9.0: \n",
      "episode:  44 score -6.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 3.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 4.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -7.0:  steps 9.0: \n",
      "episode:  45 score -7.0: steps 9.0: average_score -5.2:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "0\n",
      "action:  L score 0.0:  steps 1.0: \n",
      "0\n",
      "action:  R score 0.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 7.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  U score -6.0:  steps 9.0: \n",
      "episode:  46 score -6.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 9.0: \n",
      "episode:  47 score -4.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 1.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 2.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 3.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 6.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -3.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 9.0: \n",
      "episode:  48 score -3.0: steps 9.0: average_score -5.2:\n",
      "-1\n",
      "action:  U score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 8.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 9.0: \n",
      "episode:  49 score -3.0: steps 9.0: average_score -5.1:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  U score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -2.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -2.0:  steps 5.0: \n",
      "0\n",
      "action:  L score -2.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -3.0:  steps 7.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -5.0:  steps 9.0: \n",
      "episode:  50 score -5.0: steps 9.0: average_score -5.1:\n",
      "-1\n",
      "action:  D score -1.0:  steps 0.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  L score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "-1\n",
      "action:  D score -4.0:  steps 5.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 6.0: \n",
      "-1\n",
      "action:  U score -5.0:  steps 7.0: \n",
      "0\n",
      "action:  R score -5.0:  steps 8.0: \n",
      "-1\n",
      "action:  D score -6.0:  steps 9.0: \n",
      "episode:  51 score -6.0: steps 9.0: average_score -5.1:\n",
      "0\n",
      "action:  L score 0.0:  steps 0.0: \n",
      "-1\n",
      "action:  D score -1.0:  steps 1.0: \n",
      "0\n",
      "action:  R score -1.0:  steps 2.0: \n",
      "-1\n",
      "action:  U score -2.0:  steps 3.0: \n",
      "-1\n",
      "action:  D score -3.0:  steps 4.0: \n",
      "0\n",
      "action:  R score -3.0:  steps 5.0: \n",
      "-1\n",
      "action:  U score -4.0:  steps 6.0: \n",
      "0\n",
      "action:  R score -4.0:  steps 7.0: \n",
      "0\n",
      "action:  L score -4.0:  steps 8.0: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-17051f0b98a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;31m#TODO:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[1;31m# add heading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-17051f0b98a7>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckCollisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetIR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-17051f0b98a7>\u001b[0m in \u001b[0;36mgetIR\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckTreeDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                 \u001b[0mTotal_IR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTotal_IR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetEcho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheading\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-17051f0b98a7>\u001b[0m in \u001b[0;36mgetEcho\u001b[1;34m(self, DronePos, DroneHeading, gan)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#Generate the IRS for each leaf in range.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mgen_IRs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSpecies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAzims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mElevs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;31m# gen_IRs = gan.generator([tf.convert_to_tensor(noise,dtype='float32'),tf.convert_to_tensor(Species,dtype='float32'),tf.convert_to_tensor(np.reshape(Sizes,(Sizes.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Azims,(Azims.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Elevs,(Elevs.shape[0],1)),dtype='float32')])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# gen_IRs = gan.generator([noise,Species,Sizes,Azims,Elevs])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1214\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3822\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3824\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   3825\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1468\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1470\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Beacon - Env \n",
    "\n",
    "The tree class, the main building block in the environment.\n",
    "It loads an array of leaf positions and normals, and can generate an impulse response.\n",
    "\n",
    "Tree(pos,variety,theta)\n",
    "shift(pos)\n",
    "checkCollision(DronePos,dronesize)\n",
    "getEcho(DronePos,DroneHeading,gan)\n",
    "\"\"\"\n",
    "class Tree:\n",
    "    def __init__(self,pos,variety,theta):\n",
    "        \n",
    "        # Open and load the leaf arrays\n",
    "        f = open('eta'+str(variety)+'Out.csv')\n",
    "        csv_reader = csv.reader(f,delimiter=',')\n",
    "        \n",
    "        LeafList = []\n",
    "        for row in csv_reader:\n",
    "            LeafList.append([float(x) for x in row])\n",
    "        \n",
    "        # Separate and normalize the arrays\n",
    "        self.LeafArr = np.array(LeafList)\n",
    "        self.pos = pos\n",
    "        self.LeafPos = self.LeafArr[:,0:3]+np.array([self.pos[0],self.pos[1],0])\n",
    "        self.LeafNorm = self.LeafArr[:,3:6]\n",
    "        \n",
    "        #Rotate the leaf array\n",
    "        \n",
    "        r_left = np.eye(3)\n",
    "        # theta = np.random.choice(180)\n",
    "        r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(theta)),-np.sin(np.deg2rad(theta))],[np.sin(np.deg2rad(theta)),np.cos(np.deg2rad(theta))]])\n",
    "        com = self.LeafPos.mean(axis=0)\n",
    "        self.LeafPos = np.matmul(r_left,(self.LeafPos-com).T).T+com\n",
    "        self.LeafPos[:,2] = self.LeafPos[:,2] + 25 - np.median(self.LeafPos[:,2])\n",
    "        # self.LeafPos[:,2] = self.LeafPos[:,2] - self.LeafPos[:,2].min()\n",
    "        \n",
    "        # Knowing the max and mins can help quickly determine if the tree is in range of the drone.\n",
    "        \n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        self.radius = np.linalg.norm(np.array(self.center)-np.array([self.maxx,self.maxy]))\n",
    "        \n",
    "    \n",
    "    def shift(self,pos):\n",
    "        #Function to move the tree\n",
    "        \n",
    "        self.LeafPos = self.LeafPos + pos\n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        \n",
    "    \n",
    "    def checkCollision(self,DronePos,dronesize):\n",
    "        #Check if a position is within a certain radius of any leaves\n",
    "        \n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        if np.where(Distances < dronesize/2)[0].size > 0:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def getEcho(self,DronePos,DroneHeading,gan):\n",
    "        \n",
    "        #Get the IR of the tree given a drone position and heading, using some GAN network.\n",
    "        \n",
    "        DroneHeading = DroneHeading/np.linalg.norm(DroneHeading)\n",
    "\n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        #Filter only leaves in range\n",
    "        \n",
    "        idx = np.where(Distances < 4.3)\n",
    "        Distances = Distances[idx]\n",
    "        DroneToLeaf = DroneToLeaf[idx]\n",
    "        LeafNorm = self.LeafNorm[idx]\n",
    "        \n",
    "        \n",
    "        #Find the azimuth angle of the leaves relative to the drone.\n",
    "        \n",
    "        AngleToDrone = np.arccos(np.dot(DroneHeading,DroneToLeaf.T)/(np.linalg.norm(DroneHeading)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        import math\n",
    "        AngleToDrone=AngleToDrone*(180/math.pi)\n",
    "\n",
    "        Azims = np.arccos(np.sum(LeafNorm*DroneToLeaf,axis=1)/(np.linalg.norm(LeafNorm,axis=1)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        Azims = Azims/math.pi\n",
    "        Azims = 0.5-np.abs(0.5-Azims)\n",
    "        Azims = 2*Azims\n",
    "        \n",
    "        #Here the elevation and size are uniform random, but they could be otherwise.\n",
    "        \n",
    "        Elevs = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        \n",
    "        Sizes = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        Species = np.zeros(Azims.shape)\n",
    "    \n",
    "        #The generator needs a noise vector.\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (AngleToDrone.shape[0], gan.latent_dim))\n",
    "        \n",
    "        \n",
    "        #Generate the IRS for each leaf in range.\n",
    "        \n",
    "        gen_IRs = gan.generator.predict_on_batch([noise,Species,Sizes,Azims,Elevs])\n",
    "        # gen_IRs = gan.generator([tf.convert_to_tensor(noise,dtype='float32'),tf.convert_to_tensor(Species,dtype='float32'),tf.convert_to_tensor(np.reshape(Sizes,(Sizes.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Azims,(Azims.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Elevs,(Elevs.shape[0],1)),dtype='float32')])\n",
    "        # gen_IRs = gan.generator([noise,Species,Sizes,Azims,Elevs])\n",
    "        \n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        # gen_IRs = gen_IRs.numpy()\n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        \n",
    "        #Generate the IR for the whole tree. \n",
    "        Total_IR = np.zeros(10000)\n",
    "        \n",
    "        \n",
    "        #A few simple neccessary functions to scale and place the IRs for each leaf at the right place in the total IR.\n",
    "        \n",
    "        \n",
    "        def beam(angle):\n",
    "            if angle > 90:\n",
    "                return 0\n",
    "            return math.cos(math.radians(angle))\n",
    "            \n",
    "            \n",
    "        def timeStart(distance):\n",
    "            return int(round(400000/343.0*2*distance))\n",
    "            \n",
    "        \n",
    "        for i in range(AngleToDrone.shape[0]):\n",
    "            if timeStart(Distances[i])+400 < 10000:\n",
    "                \n",
    "                #The step that actually combines the IRs.\n",
    "                \n",
    "                Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400] = Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400]  + gen_IRs[i,:,0] * beam(AngleToDrone[i])*(1/(Distances[i]*Distances[i]))\n",
    "        \n",
    "        return Total_IR\n",
    "        \n",
    "        \n",
    "        \n",
    "# The sonar environment class.\n",
    "# Made to be similar to an OpenAI gym environment.\n",
    "\n",
    "class sonarEnv(core.Env):\n",
    "    \n",
    "    def __init__(self,ganWeights = '_2250',rotationAngle=45,speed=1,sepDist=3,dronesize=0.5):\n",
    "        \n",
    "        #Setup basic variables\n",
    "        super(sonarEnv, self).__init__()\n",
    "        \n",
    "        #4 actions (forward, back, left, right)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        #Observations are 10000 dimensional vector (the echo from the trees)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(10000,1), dtype=np.float32)\n",
    "        \n",
    "        #Radius of the drone\n",
    "        self.dronesize = dronesize\n",
    "        self.seed()\n",
    "        \n",
    "        #Load the GAN\n",
    "        self.gan = GenOnlyModel.ARGAN()\n",
    "        self.ganWeights = ganWeights\n",
    "        self.gan.load_weights(self.ganWeights)\n",
    "        \n",
    "        #How far trees are separated\n",
    "        self.sepDist = sepDist\n",
    "        \n",
    "        #Populate the world with some trees\n",
    "        self.generateInitalTrees()\n",
    "        \n",
    "        #Time\n",
    "        self.t = 0\n",
    "        \n",
    "        #Drone position\n",
    "        self.pos = np.array([0,5*self.sepDist,25])\n",
    "        \n",
    "        #Drone heading\n",
    "        self.heading=np.array([0,1,0])\n",
    "        \n",
    "        #Variables used to control the drone\n",
    "        self.done = False\n",
    "        self.r_left = np.eye(3)\n",
    "        self.r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(rotationAngle)),-np.sin(np.deg2rad(rotationAngle))],[np.sin(np.deg2rad(rotationAngle)),np.cos(np.deg2rad(rotationAngle))]])\n",
    "        self.r_right = np.eye(3)\n",
    "        self.r_right[0:2,0:2] = np.array([[np.cos(np.deg2rad(-rotationAngle)),-np.sin(np.deg2rad(-rotationAngle))],[np.sin(np.deg2rad(-rotationAngle)),np.cos(np.deg2rad(-rotationAngle))]])\n",
    "        self.speed=speed\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #This function moves the simulation forward by one timestep.\n",
    "        #The drone moves and a new observation is generated.\n",
    "        \n",
    "        self.t=self.t+1\n",
    "        \n",
    "        \n",
    "        #If we reach 10000 time steps reset\n",
    "        if self.t>10000:\n",
    "            self.done=True\n",
    "        \n",
    "        # forward -> 1 | left,right -> 0 | backwards -> -1\n",
    "        dirRew_dict = {\n",
    "            #direction(90deg) : reward\n",
    "            1 : 1,\n",
    "            0 : -1,\n",
    "            -1 : -2 \n",
    "            }\n",
    "            \n",
    "        if action == 0:\n",
    "            #Forward\n",
    "            self.pos= self.pos + self.heading*self.speed\n",
    "            reward = dirRew_dict[int(heading[1])]*self.speed\n",
    "            if self.checkCollisions():\n",
    "                self.done=True\n",
    "            self.state = self.getIR()\n",
    "        \n",
    "        elif action == 1:\n",
    "            #Back\n",
    "            self.pos=self.pos-self.heading*self.speed\n",
    "            reward = dirRew_dict[int(heading[1])]*self.speed\n",
    "            if self.checkCollisions():\n",
    "                self.done=True\n",
    "            self.state = self.getIR()\n",
    "            \n",
    "        elif action == 2:\n",
    "            #Left\n",
    "            self.heading = np.matmul(self.r_left,self.heading)\n",
    "            reward = 0\n",
    "            self.state = self.getIR()\n",
    "            \n",
    "        elif action == 3:\n",
    "            #Right\n",
    "            self.heading = np.matmul(self.r_right,self.heading)\n",
    "            reward = 0\n",
    "            self.state = self.getIR()\n",
    "            \n",
    "        print(reward)\n",
    "        \n",
    "        #This moves the trees and makes a new row if the ronde has moved forward enough\n",
    "        self.checkTreeRow()\n",
    "        \n",
    "        #return np.append(np.array(self.state), self.heading,axis=None), reward, self.done, {}\n",
    "        return np.array(self.state), reward, self.done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        #Reset the environment\n",
    "        self.t = 0\n",
    "        self.pos = np.array([5,5,25])\n",
    "        self.heading=np.array([0,1,0])\n",
    "        self.done = False\n",
    "        self.generateInitalTrees()\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "        # self.gan.close()\n",
    "        # del self.gan\n",
    "        # keras.backend.clear_session()\n",
    "        # gc.collect()\n",
    "        # self.gan = ARGANmodel.ARGAN()\n",
    "        # self.gan.load_weights(self.ganWeights)\n",
    "        return self.state\n",
    "        \n",
    "    #The render functions all generate images, they have different properties and were ad hoc added as needed. Technically not necessary \n",
    "    \n",
    "    def render(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lg = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lr = t.LeafPos[idx1]\n",
    "            plt.plot(lr[:,0],lr[:,1],'r.')\n",
    "            plt.plot(lg[:,0],lg[:,1],'g.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                plt.plot(t.center[0],t.center[1],'g*')\n",
    "            else:\n",
    "                plt.plot(t.center[0],t.center[1],'r*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lg = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lr = t.LeafPos[idx1]\n",
    "            plt.plot(lr[:,0],lr[:,1],'r.')\n",
    "            plt.plot(lg[:,0],lg[:,1],'g.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                plt.plot(t.center[0],t.center[1],'g*')\n",
    "            else:\n",
    "                plt.plot(t.center[0],t.center[1],'r*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-6,self.pos[1]+6])\n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_step_'+str(i)+'.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        plt.plot(self.state)\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_step_'+str(i)+'_observedIR.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def render_for_nips(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        # plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state)\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()    \n",
    "    \n",
    "\n",
    "    def render_for_ave(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        # plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state,color=(232/256, 119/256, 34/256))\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.gan.close()\n",
    "        \n",
    "    def generateInitalTrees(self):\n",
    "        #Makes two rows of trees\n",
    "        \n",
    "        self.TreeRow1 = []\n",
    "        self.TreeRow2 = []\n",
    "        skip1 = np.random.choice(10)\n",
    "        skip2 = np.random.choice(10)\n",
    "        for i in range(10):\n",
    "            if i != skip1:\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            if i != skip2:\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "    def checkTreeRow(self):\n",
    "        #If you've gone past one row, make another row in front\n",
    "        \n",
    "        if self.pos[1] > 10+1:\n",
    "            self.pos[1] = self.pos[1] - 10\n",
    "            self.TreeRow1 = self.TreeRow2\n",
    "            for t in self.TreeRow1:\n",
    "                t.shift(np.array((0,-10,0)))\n",
    "            self.TreeRow2 = []\n",
    "            skip2 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip2:\n",
    "                    self.TreeRow2.append(Tree((i*3,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "        if self.pos[1] < -1:\n",
    "            self.pos[1] = self.pos[1] +10\n",
    "            self.TreeRow2 = self.TreeRow1\n",
    "            for t in self.TreeRow2:\n",
    "                t.shift(np.array((0,10,0)))\n",
    "            self.TreeRow1 = []\n",
    "            skip1 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip1:\n",
    "                    self.TreeRow1.append(Tree((i*3,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            \n",
    "                    \n",
    "        \n",
    "        if self.pos[0] < 0:\n",
    "            self.pos[0] = self.pos[0] +10*self.sepDist\n",
    "        if self.pos[0] > 10*self.sepDist:\n",
    "            self.pos[0] = self.pos[0] -10*self.sepDist\n",
    "    \n",
    "    def getIR(self):\n",
    "        \n",
    "        #Sum the IRs together from all the trees in range\n",
    "        Total_IR = np.zeros(10000)\n",
    "        for tree in self.TreeRow1:\n",
    "            if self.checkTreeDist(tree):\n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        for tree in self.TreeRow2:\n",
    "            if self.checkTreeDist(tree):\n",
    "                \n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        \n",
    "        return Total_IR\n",
    "    \n",
    "    def checkTreeDist(self,tree):\n",
    "        if np.linalg.norm((self.pos[0:2]-tree.center)) - tree.radius < 4.3:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def checkCollisions(self):\n",
    "        #Check if there is a collision with any trees\n",
    "        \n",
    "        for tree in self.TreeRow1:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        for tree in self.TreeRow2:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "from keras.layers import Dense,Activation, Input, concatenate, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        \n",
    "        #env2d = Input(shape=(self.input_dims,self.input_dims))\n",
    "        env1d = Input(shape=(self.input_dims))\n",
    "        env = Flatten()(env1d)\n",
    "        \n",
    "        advantages = Input(shape=[1])\n",
    "        \n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(env)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        dense3 = Dense(self.fc2_dims, activation='relu')(dense2)\n",
    "        dense4 = Dense(self.fc2_dims, activation='relu')(dense3)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense4)\n",
    "       \n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        \n",
    "        policy = Model(inputs=[env1d,advantages], outputs=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        \n",
    "        self.predict = Model(inputs=[env1d], outputs=[probs])\n",
    "        \n",
    "        return policy, self.predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        \n",
    "        #norm = np.linalg.norm(state)\n",
    "        #norm_state = state/norm\n",
    "        \n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        state = observation\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(state)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "       \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory ,self.G], action_memory)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    #return cost funtion\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "    \n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = sonarEnv(rotationAngle=90,speed=1,sepDist=3,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10000, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    strAction = ''\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 10 :\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "        if action == 0:\n",
    "            strAction = 'U'\n",
    "        elif action == 1:\n",
    "            strAction = 'D'\n",
    "        elif action == 2:\n",
    "            strAction = 'L'\n",
    "        elif action == 3:\n",
    "            strAction = 'R'\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        #env.render(steps)\n",
    "        print('action: ', strAction, 'score %.1f: ' % score,'steps %.1f: ' % (steps-1))\n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "    \n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query - heading  reward plot\n",
    "rotaion angle 90\n",
    "\"\"\"\n",
    "heading = np.array([0,1,0])\n",
    "rotationAngle = 90\n",
    "\n",
    "r_left = np.eye(3)\n",
    "r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(rotationAngle)),-np.sin(np.deg2rad(rotationAngle))],[np.sin(np.deg2rad(rotationAngle)),np.cos(np.deg2rad(rotationAngle))]])\n",
    "r_right = np.eye(3)\n",
    "r_right[0:2,0:2] = np.array([[np.cos(np.deg2rad(-rotationAngle)),-np.sin(np.deg2rad(-rotationAngle))],[np.sin(np.deg2rad(-rotationAngle)),np.cos(np.deg2rad(-rotationAngle))]])\n",
    "  \n",
    "\n",
    "#\n",
    "#heading = np.matmul(r_right,heading)\n",
    "y = list()\n",
    "x = list()\n",
    "\n",
    "# forward -> 1 | left,right -> 0 | backwards -> -1\n",
    "dirRew_dict = {\n",
    "        #direction(90deg) : reward\n",
    "        1 : 1,\n",
    "        -1 : -2,\n",
    "        0 : -1\n",
    "    }\n",
    "\n",
    "for i in range(9):\n",
    "    y.append(dirRew_dict[int(heading[1])])\n",
    "    x.append(i)\n",
    "    heading = np.matmul(r_left,heading)\n",
    "    \n",
    "plt.plot(x, y);\n",
    "print(heading)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "North Edge - env\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN - agent \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 15)        60          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 15)        240         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 15)           30          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 15)        60          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15)           60          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 15)        240         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 15)           240         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 15)        60          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           20          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15)           60          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 15)        240         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10)           40          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 15)           240         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 15)        60          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 50)           2550        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           110         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15)           60          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           20          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 15, 1)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 50, 1)        0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10)           40          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 15)           240         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 10)           40          dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 65, 1)        0           reshape[0][0]                    \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 1)        0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15)           60          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           110         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 1)        0           concatenate[0][0]                \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 15, 1)        0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 10)           40          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 90, 1)        0           concatenate_1[0][0]              \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 10, 1)        0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 1)       0           concatenate_2[0][0]              \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          10100       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 100)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 100)          400         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          10100       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 100)          0           flatten[0][0]                    \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 100)          0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 100)          400         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 200)          20200       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 100, 1)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 200)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d (UpSampling1D)    (None, 200, 1)       0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 200)          800         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 200)       0           up_sampling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 200)          40200       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 200)       0           reshape_6[0][0]                  \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1, 200)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1, 200)       800         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 200)       40200       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1, 200)       0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1, 200)       800         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 200)       40200       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 200)       0           batch_normalization_14[0][0]     \n",
      "                                                                 dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1, 200)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1, 200)       800         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 200)       40200       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1, 200)       0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1, 200)       800         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1, 200)       40200       batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 200)       0           batch_normalization_16[0][0]     \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1, 200)       0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1, 200)       800         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1, 400)       80400       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 200, 1)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 1, 400)       0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1D)  (None, 400, 1)       0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 1, 400)       1600        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 400)       0           up_sampling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1, 400)       160400      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1, 400)       0           reshape_8[0][0]                  \n",
      "                                                                 dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 1, 400)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 1, 400)       1600        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 400)       160400      batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 1, 400)       0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1, 400)       1600        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1, 400)       160400      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 1, 400)       0           batch_normalization_20[0][0]     \n",
      "                                                                 dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 1, 400)       0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1, 400)       1600        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1, 400)       160400      batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 1, 400)       0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1, 400)       1600        leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1, 400)       160400      batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 1, 400)       0           batch_normalization_22[0][0]     \n",
      "                                                                 dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 1, 400)       0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 1, 400)       1600        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1, 400)       160400      batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 400, 1)       0           dense_26[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,304,320\n",
      "Trainable params: 1,296,430\n",
      "Non-trainable params: 7,890\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sefun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "action:  R       score:    0.0  steps:    0.0 \n",
      "action:  R       score:    0.0  steps:    1.0 \n",
      "action:  U       score:   -2.0  steps:    2.0 \n",
      "ep:      0  score :   -2.0  steps : 2.0    average_score :   -2.0\n",
      "action:  R       score:    0.0  steps:    0.0 \n",
      "action:  L       score:    0.0  steps:    1.0 \n",
      "action:  R       score:    0.0  steps:    2.0 \n",
      "action:  L       score:    0.0  steps:    3.0 \n",
      "action:  L       score:    0.0  steps:    4.0 \n",
      "action:  L       score:    0.0  steps:    5.0 \n",
      "action:  L       score:    0.0  steps:    6.0 \n",
      "action:  D       score:   -1.0  steps:    7.0 \n",
      "action:  U       score:   -2.0  steps:    8.0 \n",
      "action:  D       score:   -3.0  steps:    9.0 \n",
      "ep:      1  score :   -3.0  steps : 9.0    average_score :   -2.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c9f67e0bcad2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m     \u001b[0mstrAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     \u001b[1;31m#env.setPrefAction()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c9f67e0bcad2>\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheading\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerateInitalTrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetIR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c9f67e0bcad2>\u001b[0m in \u001b[0;36mgenerateInitalTrees\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mskip2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeRow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msepDist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeRow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msepDist\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msepDist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeRow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msepDist\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msepDist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c9f67e0bcad2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pos, variety, theta)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mLeafList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mLeafList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla World- \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy import sin, cos, pi\n",
    "\n",
    "from gym import core, spaces\n",
    "from gym.utils import seeding\n",
    "import csv\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import GenOnlyModel\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "import gc\n",
    "import keras.backend\n",
    "\n",
    "import objgraph\n",
    "\n",
    "a = 1\n",
    "\n",
    "# The tree class, the main building block in the environment.\n",
    "# It loads an array of leaf positions and normals, and can generate an impulse response.\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self,pos,variety,theta):\n",
    "        \n",
    "        # Open and load the leaf arrays\n",
    "        f = open('eta'+str(variety)+'Out.csv')\n",
    "        csv_reader = csv.reader(f,delimiter=',')\n",
    "        \n",
    "        LeafList = []\n",
    "        for row in csv_reader:\n",
    "            LeafList.append([float(x) for x in row])\n",
    "        \n",
    "        # Separate and normalize the arrays\n",
    "        self.LeafArr = np.array(LeafList)\n",
    "        self.pos = pos\n",
    "        self.LeafPos = self.LeafArr[:,0:3]+np.array([self.pos[0],self.pos[1],0])\n",
    "        self.LeafNorm = self.LeafArr[:,3:6]\n",
    "        \n",
    "        #Rotate the leaf array\n",
    "        \n",
    "        r_left = np.eye(3)\n",
    "        # theta = np.random.choice(180)\n",
    "        r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(theta)),-np.sin(np.deg2rad(theta))],[np.sin(np.deg2rad(theta)),np.cos(np.deg2rad(theta))]])\n",
    "        com = self.LeafPos.mean(axis=0)\n",
    "        self.LeafPos = np.matmul(r_left,(self.LeafPos-com).T).T+com\n",
    "        self.LeafPos[:,2] = self.LeafPos[:,2] + 25 - np.median(self.LeafPos[:,2])\n",
    "        # self.LeafPos[:,2] = self.LeafPos[:,2] - self.LeafPos[:,2].min()\n",
    "        \n",
    "        # Knowing the max and mins can help quickly determine if the tree is in range of the drone.\n",
    "        \n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        self.radius = np.linalg.norm(np.array(self.center)-np.array([self.maxx,self.maxy]))\n",
    "        \n",
    "    \n",
    "    def shift(self,pos):\n",
    "        #Function to move the tree\n",
    "        \n",
    "        self.LeafPos = self.LeafPos + pos\n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        \n",
    "    \n",
    "    def checkCollision(self,DronePos,dronesize):\n",
    "        #Check if a position is within a certain radius of any leaves\n",
    "        \n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        if np.where(Distances < dronesize/2)[0].size > 0:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def getEcho(self,DronePos,DroneHeading,gan):\n",
    "        \n",
    "        #Get the IR of the tree given a drone position and heading, using some GAN network.\n",
    "        \n",
    "        DroneHeading = DroneHeading/np.linalg.norm(DroneHeading)\n",
    "\n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        #Filter only leaves in range\n",
    "        \n",
    "        idx = np.where(Distances < 4.3)\n",
    "        Distances = Distances[idx]\n",
    "        DroneToLeaf = DroneToLeaf[idx]\n",
    "        LeafNorm = self.LeafNorm[idx]\n",
    "        \n",
    "        \n",
    "        #Find the azimuth angle of the leaves relative to the drone.\n",
    "        \n",
    "        AngleToDrone = np.arccos(np.dot(DroneHeading,DroneToLeaf.T)/(np.linalg.norm(DroneHeading)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        import math\n",
    "        AngleToDrone=AngleToDrone*(180/math.pi)\n",
    "\n",
    "        Azims = np.arccos(np.sum(LeafNorm*DroneToLeaf,axis=1)/(np.linalg.norm(LeafNorm,axis=1)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        Azims = Azims/math.pi\n",
    "        Azims = 0.5-np.abs(0.5-Azims)\n",
    "        Azims = 2*Azims\n",
    "        \n",
    "        #Here the elevation and size are uniform random, but they could be otherwise.\n",
    "        \n",
    "        Elevs = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        \n",
    "        Sizes = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        Species = np.zeros(Azims.shape)\n",
    "    \n",
    "        #The generator needs a noise vector.\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (AngleToDrone.shape[0], gan.latent_dim))\n",
    "        \n",
    "        \n",
    "        #Generate the IRS for each leaf in range.\n",
    "        \n",
    "        gen_IRs = gan.generator.predict_on_batch([noise,Species,Sizes,Azims,Elevs])\n",
    "        # gen_IRs = gan.generator([tf.convert_to_tensor(noise,dtype='float32'),tf.convert_to_tensor(Species,dtype='float32'),tf.convert_to_tensor(np.reshape(Sizes,(Sizes.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Azims,(Azims.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Elevs,(Elevs.shape[0],1)),dtype='float32')])\n",
    "        # gen_IRs = gan.generator([noise,Species,Sizes,Azims,Elevs])\n",
    "        \n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        # gen_IRs = gen_IRs.numpy()\n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        \n",
    "        #Generate the IR for the whole tree. \n",
    "        Total_IR = np.zeros(10000)\n",
    "        \n",
    "        \n",
    "        #A few simple neccessary functions to scale and place the IRs for each leaf at the right place in the total IR.\n",
    "        \n",
    "        \n",
    "        def beam(angle):\n",
    "            if angle > 90:\n",
    "                return 0\n",
    "            return math.cos(math.radians(angle))\n",
    "            \n",
    "            \n",
    "        def timeStart(distance):\n",
    "            return int(round(400000/343.0*2*distance))\n",
    "            \n",
    "        \n",
    "        for i in range(AngleToDrone.shape[0]):\n",
    "            if timeStart(Distances[i])+400 < 10000:\n",
    "                \n",
    "                #The step that actually combines the IRs.\n",
    "                \n",
    "                Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400] = Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400]  + gen_IRs[i,:,0] * beam(AngleToDrone[i])*(1/(Distances[i]*Distances[i]))\n",
    "        \n",
    "        return Total_IR\n",
    "        \n",
    "        \n",
    "        \n",
    "# The sonar environment class.\n",
    "# Made to be similar to an OpenAI gym environment.\n",
    "\n",
    "class sonarEnv(core.Env):\n",
    "    \n",
    "    def __init__(self,ganWeights = '_2250',rotationAngle=45,speed=1,sepDist=3,dronesize=0.5):\n",
    "        \n",
    "        #Setup basic variables\n",
    "        super(sonarEnv, self).__init__()\n",
    "        \n",
    "        #4 actions (forward, back, left, right)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        #Observations are 10000 dimensional vector (the echo from the trees)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(10000,1), dtype=np.float32)\n",
    "        \n",
    "        #Radius of the drone\n",
    "        self.dronesize = dronesize\n",
    "        self.seed()\n",
    "        \n",
    "        #Load the GAN\n",
    "        self.gan = GenOnlyModel.ARGAN()\n",
    "        self.ganWeights = ganWeights\n",
    "        self.gan.load_weights(self.ganWeights)\n",
    "        \n",
    "        #How far trees are separated\n",
    "        self.sepDist = sepDist\n",
    "        \n",
    "        #Populate the world with some trees\n",
    "        self.generateInitalTrees()\n",
    "        \n",
    "        #Time\n",
    "        self.t = 0\n",
    "        \n",
    "        #Drone position\n",
    "        self.pos = np.array([0,5*self.sepDist,25])\n",
    "        \n",
    "        #Drone heading\n",
    "        self.heading=np.array([0,1,0])\n",
    "        \n",
    "        #Variables used to control the drone\n",
    "        self.done = False\n",
    "        self.r_left = np.eye(3)\n",
    "        self.r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(rotationAngle)),-np.sin(np.deg2rad(rotationAngle))],[np.sin(np.deg2rad(rotationAngle)),np.cos(np.deg2rad(rotationAngle))]])\n",
    "        self.r_right = np.eye(3)\n",
    "        self.r_right[0:2,0:2] = np.array([[np.cos(np.deg2rad(-rotationAngle)),-np.sin(np.deg2rad(-rotationAngle))],[np.sin(np.deg2rad(-rotationAngle)),np.cos(np.deg2rad(-rotationAngle))]])\n",
    "        self.speed=speed\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #This function moves the simulation forward by one timestep.\n",
    "        #The drone moves and a new observation is generated.\n",
    "        \n",
    "        self.t=self.t+1\n",
    "        \n",
    "        \n",
    "        #If we reach 10000 time steps reset\n",
    "        if self.t>10000:\n",
    "            self.done=True\n",
    "        \n",
    "        # forward -> 1 | left,right -> 0 | backwards -> -1\n",
    "        dirRew_dict = {\n",
    "            #direction(90deg) : reward\n",
    "            1 : 1,\n",
    "            0 : -1,\n",
    "            -1: -2 \n",
    "            }\n",
    "            \n",
    "        if action == 0:\n",
    "            #Forward\n",
    "            self.pos= self.pos + self.heading*self.speed\n",
    "            reward = dirRew_dict[int(self.heading[1])]*self.speed\n",
    "            if self.checkCollisions():\n",
    "                self.done=True\n",
    "            self.state = self.getIR()\n",
    "        \n",
    "        elif action == 1:\n",
    "            #Back\n",
    "            self.pos=self.pos-self.heading*self.speed\n",
    "            reward = dirRew_dict[int(self.heading[1])]*self.speed\n",
    "            if self.checkCollisions():\n",
    "                self.done=True\n",
    "            self.state = self.getIR()\n",
    "            \n",
    "        elif action == 2:\n",
    "            #Left\n",
    "            self.heading = np.matmul(self.r_left,self.heading)\n",
    "            reward = 0\n",
    "            self.state = self.getIR()\n",
    "            \n",
    "        elif action == 3:\n",
    "            #Right\n",
    "            self.heading = np.matmul(self.r_right,self.heading)\n",
    "            reward = 0\n",
    "            self.state = self.getIR()\n",
    "        \n",
    "        #This moves the trees and makes a new row if the ronde has moved forward enough\n",
    "        self.checkTreeRow()\n",
    "        \n",
    "        #return np.append(np.array(self.state), self.heading,axis=None), reward, self.done, {}\n",
    "        return np.array(self.state), reward, self.done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        #Reset the environment\n",
    "        self.t = 0\n",
    "        self.pos = np.array([5,5,25])\n",
    "        self.heading=np.array([0,1,0])\n",
    "        self.done = False\n",
    "        self.generateInitalTrees()\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "        # self.gan.close()\n",
    "        # del self.gan\n",
    "        # keras.backend.clear_session()\n",
    "        # gc.collect()\n",
    "        # self.gan = ARGANmodel.ARGAN()\n",
    "        # self.gan.load_weights(self.ganWeights)\n",
    "        return self.state\n",
    "        \n",
    "    #The render functions all generate images, they have different properties and were ad hoc added as needed. Technically not necessary \n",
    "    \n",
    "    def render(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lg = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lr = t.LeafPos[idx1]\n",
    "            plt.plot(lr[:,0],lr[:,1],'r.')\n",
    "            plt.plot(lg[:,0],lg[:,1],'g.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                plt.plot(t.center[0],t.center[1],'g*')\n",
    "            else:\n",
    "                plt.plot(t.center[0],t.center[1],'r*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lg = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lr = t.LeafPos[idx1]\n",
    "            plt.plot(lr[:,0],lr[:,1],'r.')\n",
    "            plt.plot(lg[:,0],lg[:,1],'g.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                plt.plot(t.center[0],t.center[1],'g*')\n",
    "            else:\n",
    "                plt.plot(t.center[0],t.center[1],'r*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-6,self.pos[1]+6])\n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_step_'+str(i)+'.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        plt.plot(self.state)\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_step_'+str(i)+'_observedIR.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def render_for_nips(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        # plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state)\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()    \n",
    "    \n",
    "\n",
    "    def render_for_ave(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        # plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state,color=(232/256, 119/256, 34/256))\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.gan.close()\n",
    "        \n",
    "    def generateInitalTrees(self):\n",
    "        #Makes two rows of trees\n",
    "        \n",
    "        self.TreeRow1 = []\n",
    "        self.TreeRow2 = []\n",
    "        skip1 = np.random.choice(10)\n",
    "        skip2 = np.random.choice(10)\n",
    "        for i in range(10):\n",
    "            if i != skip1:\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            if i != skip2:\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "    def checkTreeRow(self):\n",
    "        #If you've gone past one row, make another row in front\n",
    "        \n",
    "        if self.pos[1] > 10+1:\n",
    "            self.pos[1] = self.pos[1] - 10\n",
    "            self.TreeRow1 = self.TreeRow2\n",
    "            for t in self.TreeRow1:\n",
    "                t.shift(np.array((0,-10,0)))\n",
    "            self.TreeRow2 = []\n",
    "            skip2 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip2:\n",
    "                    self.TreeRow2.append(Tree((i*3,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "        if self.pos[1] < -1:\n",
    "            self.pos[1] = self.pos[1] +10\n",
    "            self.TreeRow2 = self.TreeRow1\n",
    "            for t in self.TreeRow2:\n",
    "                t.shift(np.array((0,10,0)))\n",
    "            self.TreeRow1 = []\n",
    "            skip1 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip1:\n",
    "                    self.TreeRow1.append(Tree((i*3,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            \n",
    "                    \n",
    "        \n",
    "        if self.pos[0] < 0:\n",
    "            self.pos[0] = self.pos[0] +10*self.sepDist\n",
    "        if self.pos[0] > 10*self.sepDist:\n",
    "            self.pos[0] = self.pos[0] -10*self.sepDist\n",
    "    \n",
    "    def getIR(self):\n",
    "        \n",
    "        #Sum the IRs together from all the trees in range\n",
    "        Total_IR = np.zeros(10000)\n",
    "        for tree in self.TreeRow1:\n",
    "            if self.checkTreeDist(tree):\n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        for tree in self.TreeRow2:\n",
    "            if self.checkTreeDist(tree):\n",
    "                \n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        \n",
    "        return Total_IR\n",
    "    \n",
    "    def checkTreeDist(self,tree):\n",
    "        if np.linalg.norm((self.pos[0:2]-tree.center)) - tree.radius < 4.3:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def checkCollisions(self):\n",
    "        #Check if there is a collision with any trees\n",
    "        \n",
    "        for tree in self.TreeRow1:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        for tree in self.TreeRow2:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "from keras.layers import Dense,Activation, Input, concatenate, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        \n",
    "        #env2d = Input(shape=(self.input_dims,self.input_dims))\n",
    "        env1d = Input(shape=(self.input_dims))\n",
    "        env = Flatten()(env1d)\n",
    "        \n",
    "        advantages = Input(shape=[1])\n",
    "        \n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(env)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        dense3 = Dense(self.fc2_dims, activation='relu')(dense2)\n",
    "        dense4 = Dense(self.fc2_dims, activation='relu')(dense3)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense4)\n",
    "       \n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        \n",
    "        policy = Model(inputs=[env1d,advantages], outputs=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        \n",
    "        self.predict = Model(inputs=[env1d], outputs=[probs])\n",
    "        \n",
    "        return policy, self.predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        \n",
    "        #norm = np.linalg.norm(state)\n",
    "        #norm_state = state/norm\n",
    "        \n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        state = observation\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(state)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "       \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory ,self.G], action_memory)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    #return cost funtion\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = sonarEnv(rotationAngle=90,speed=1,sepDist=5,dronesize=2)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10000, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 10000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    strAction = ''\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 10 :\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "        if action == 0:\n",
    "            strAction = 'U'\n",
    "        elif action == 1:\n",
    "            strAction = 'D'\n",
    "        elif action == 2:\n",
    "            strAction = 'L'\n",
    "        elif action == 3:\n",
    "            strAction = 'R'\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        #env.render(steps)\n",
    "        print('action:  %-6s ' % strAction, 'score: %6.1f ' % score,'steps: %6.1f ' % (steps-1))\n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    agent.learn()\n",
    "    \n",
    "    print('ep: %6.1d' % i, ' score : %6.1f' % score,' steps : %-6.1f' % (steps-1),'average_score : %6.1f' % np.mean(score_history[-100:]))\n",
    "    \n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LineCollection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1d7d9a7ed89b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcoll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineCollection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgist_ncar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mcoll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LineCollection' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def uniqueish_color():\n",
    "    \"\"\"There're better ways to generate unique colors, but this isn't awful.\"\"\"\n",
    "    return plt.cm.gist_ncar(np.random.random())\n",
    "\n",
    "\n",
    "score_arr = np.array([i for i in range(10)])\n",
    "steps_arr = np.array([i for i in range(10)])\n",
    "score_step_history = np.array([steps_arr, score_arr])\n",
    "# Reshape things so that we have a sequence of:\n",
    "# [[(x0,y0),(x1,y1)],[(x0,y0),(x1,y1)],...]\n",
    "score_step_history = score_step_history.reshape(-1, 1, 2)\n",
    "segments = np.hstack([score_step_history[:-1], score_step_history[1:]])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "coll = LineCollection(segments, cmap=plt.cm.gist_ncar)\n",
    "coll.set_array(np.random.random(xy.shape[0]))\n",
    "\n",
    "ax.add_collection(coll)\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Inputs,Compass_heading] - agent\n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = ENV.sonarEnv(speed=0.5,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10003, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 500:\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        env.render(steps)\n",
    "        \n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    \n",
    "   \n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "          'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Path Construction - agent\n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = ENV.sonarEnv(speed=0.5,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10003, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 500:\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        env.render(steps)\n",
    "        \n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    \n",
    "   \n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "          'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
