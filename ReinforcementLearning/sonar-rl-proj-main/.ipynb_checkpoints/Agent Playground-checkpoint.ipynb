{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "n_steps = 30\n",
    "n_episodes = 3000\n",
    "num_inputs = 10000\n",
    "num_actions = 4\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "filename = \"a2c_7L_Weights_final\"\n",
    "agent = Agent(ALPHA=learning_rate, GAMMA=gamma, input_dims=num_inputs, n_actions=num_actions,load = True, fname_policy=str(\"models/\"+filename+\"_policy.h5\"))\n",
    "\n",
    "env = ENV.sonarEnv(rotationAngle=90)\n",
    "\n",
    "#steps record\n",
    "all_lengths = []\n",
    "average_lengths = []\n",
    "steps_history = []\n",
    "#step direction record\n",
    "stepDirct_history = [0,0,0,0]\n",
    "action_Dict = {\n",
    "    0 : 'U',\n",
    "    1 : 'D',\n",
    "    2 : 'L',\n",
    "    3 : 'R'\n",
    "}\n",
    "#reward record\n",
    "all_rewards = []\n",
    "score_history = []\n",
    "avg_score_history = []\n",
    "\n",
    "entropy_term = 0\n",
    "\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    steps = 0\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    agent.reset_ep_graph()\n",
    "    #start = time.time() #4sec-5steps 11sec-20steps 21sec-30steps\n",
    "    while not done and steps < n_steps:\n",
    "        #select action\n",
    "        action = agent.choose_action(state)\n",
    "        #record direction\n",
    "        stepDirct_history[action] += 1\n",
    "        #step\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #store transitions: state, action, reward, self.log_prob in self.log_probs, self.log_prob in self.log_probs\n",
    "        steps += 1\n",
    "        rewards.append(reward)\n",
    "        agent.store_transition(state,action, reward, steps)\n",
    "        state = new_state\n",
    "        \n",
    "        if(i%500 == 0):\n",
    "            env.render(steps,i,reward)      \n",
    "    all_rewards.append(np.sum(rewards))\n",
    "    all_lengths.append(steps)\n",
    "    agent.learn()\n",
    "    if(i%10== 0):\n",
    "        if(i%100== 0):\n",
    "            agent.plot_ep_graph(i, all_rewards,stepDirct_history)\n",
    "        agent.print_ep_stats(all_rewards)\n",
    "        print('ep: %6.1d' % i, ' score : %6.1f' % score,' steps : %-6.1f' % (steps-1),\n",
    "          'average_score : %5.2f' % np.mean(all_rewards[-10:]),\n",
    "          'F:%4.1f' % (stepDirct_history[0]/sum(stepDirct_history)),\n",
    "          'B:%4.1f' % (stepDirct_history[1]/sum(stepDirct_history)),\n",
    "          'L:%4.1f' % (stepDirct_history[2]/sum(stepDirct_history)),\n",
    "          'R:%4.1f' %(stepDirct_history[3]/sum(stepDirct_history)))\n",
    "    \n",
    "    stepDirct_history = [0,0,0,0] \n",
    "    avg_score_history.append(np.mean(score_history[-10:]))\n",
    "    if (i%500 == 0):\n",
    "        agent.save_model()\n",
    "\n",
    "# transfer function to agnent class\n",
    "env.plot_results(avg_score_history, steps_arr,filename)\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sin, cos, pi\n",
    "\n",
    "from gym import core, spaces\n",
    "from gym.utils import seeding\n",
    "import csv\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import GenOnlyModel\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "import gc\n",
    "import keras.backend\n",
    "\n",
    "import objgraph\n",
    "\n",
    "a = 1\n",
    "\n",
    "# The tree class, the main building block in the environment.\n",
    "# It loads an array of leaf positions and normals, and can generate an impulse response.\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self,pos,variety,theta):\n",
    "        \n",
    "        # Open and load the leaf arrays\n",
    "        f = open('eta'+str(variety)+'Out.csv')\n",
    "        csv_reader = csv.reader(f,delimiter=',')\n",
    "        \n",
    "        LeafList = []\n",
    "        for row in csv_reader:\n",
    "            LeafList.append([float(x) for x in row])\n",
    "        \n",
    "        # Separate and normalize the arrays\n",
    "        self.LeafArr = np.array(LeafList)\n",
    "        self.pos = pos\n",
    "        self.LeafPos = self.LeafArr[:,0:3]+np.array([self.pos[0],self.pos[1],0])\n",
    "        self.LeafNorm = self.LeafArr[:,3:6]\n",
    "        \n",
    "        #Rotate the leaf array\n",
    "        \n",
    "        r_left = np.eye(3)\n",
    "        # theta = np.random.choice(180)\n",
    "        r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(theta)),-np.sin(np.deg2rad(theta))],[np.sin(np.deg2rad(theta)),np.cos(np.deg2rad(theta))]])\n",
    "        com = self.LeafPos.mean(axis=0)\n",
    "        self.LeafPos = np.matmul(r_left,(self.LeafPos-com).T).T+com\n",
    "        self.LeafPos[:,2] = self.LeafPos[:,2] + 25 - np.median(self.LeafPos[:,2])\n",
    "        # self.LeafPos[:,2] = self.LeafPos[:,2] - self.LeafPos[:,2].min()\n",
    "        \n",
    "        # Knowing the max and mins can help quickly determine if the tree is in range of the drone.\n",
    "        \n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        self.radius = np.linalg.norm(np.array(self.center)-np.array([self.maxx,self.maxy]))\n",
    "        \n",
    "    \n",
    "    def shift(self,pos):\n",
    "        #Function to move the tree\n",
    "        self.LeafPos = self.LeafPos + pos\n",
    "        self.maxx = self.LeafPos[:,0].max()\n",
    "        self.minx = self.LeafPos[:,0].min()\n",
    "        self.maxy = self.LeafPos[:,1].max()\n",
    "        self.miny = self.LeafPos[:,1].min()\n",
    "        self.center = ((self.maxx+self.minx)/2,(self.maxy+self.miny)/2)\n",
    "        \n",
    "    \n",
    "    def checkCollision(self,DronePos,dronesize):\n",
    "        #Check if a position is within a certain radius of any leaves\n",
    "        \n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        if np.where(Distances < dronesize/2)[0].size > 0:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def getEcho(self,DronePos,DroneHeading,gan):\n",
    "        \n",
    "        #Get the IR of the tree given a drone position and heading, using some GAN network.\n",
    "        \n",
    "        DroneHeading = DroneHeading/np.linalg.norm(DroneHeading)\n",
    "\n",
    "        DroneToLeaf = self.LeafPos-DronePos\n",
    "        Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "        \n",
    "        #Filter only leaves in range\n",
    "        \n",
    "        idx = np.where(Distances < 4.3)\n",
    "        Distances = Distances[idx]\n",
    "        DroneToLeaf = DroneToLeaf[idx]\n",
    "        LeafNorm = self.LeafNorm[idx]\n",
    "        \n",
    "        \n",
    "        #Find the azimuth angle of the leaves relative to the drone.\n",
    "        \n",
    "        AngleToDrone = np.arccos(np.dot(DroneHeading,DroneToLeaf.T)/(np.linalg.norm(DroneHeading)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        import math\n",
    "        AngleToDrone=AngleToDrone*(180/math.pi)\n",
    "\n",
    "        Azims = np.arccos(np.sum(LeafNorm*DroneToLeaf,axis=1)/(np.linalg.norm(LeafNorm,axis=1)*np.linalg.norm(DroneToLeaf,axis=1)))\n",
    "        Azims = Azims/math.pi\n",
    "        Azims = 0.5-np.abs(0.5-Azims)\n",
    "        Azims = 2*Azims\n",
    "        \n",
    "        #Here the elevation and size are uniform random, but they could be otherwise.\n",
    "        \n",
    "        Elevs = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        \n",
    "        Sizes = np.random.uniform(0.1,1.0,size=Azims.shape)\n",
    "        Species = np.zeros(Azims.shape)\n",
    "    \n",
    "        #The generator needs a noise vector.\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (AngleToDrone.shape[0], gan.latent_dim))\n",
    "        \n",
    "        \n",
    "        #Generate the IRS for each leaf in range.\n",
    "        \n",
    "        gen_IRs = gan.generator.predict_on_batch([noise,Species,Sizes,Azims,Elevs])\n",
    "        # gen_IRs = gan.generator([tf.convert_to_tensor(noise,dtype='float32'),tf.convert_to_tensor(Species,dtype='float32'),tf.convert_to_tensor(np.reshape(Sizes,(Sizes.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Azims,(Azims.shape[0],1)),dtype='float32'),tf.convert_to_tensor(np.reshape(Elevs,(Elevs.shape[0],1)),dtype='float32')])\n",
    "        # gen_IRs = gan.generator([noise,Species,Sizes,Azims,Elevs])\n",
    "        \n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        # gen_IRs = gen_IRs.numpy()\n",
    "        # print(gen_IRs)\n",
    "        # print(gen_IRs.shape)\n",
    "        \n",
    "        \n",
    "        #Generate the IR for the whole tree. \n",
    "        Total_IR = np.zeros(10000)\n",
    "        \n",
    "        \n",
    "        #A few simple neccessary functions to scale and place the IRs for each leaf at the right place in the total IR.\n",
    "        \n",
    "        \n",
    "        def beam(angle):\n",
    "            if angle > 90:\n",
    "                return 0\n",
    "            return math.cos(math.radians(angle))\n",
    "            \n",
    "            \n",
    "        def timeStart(distance):\n",
    "            return int(round(400000/343.0*2*distance))\n",
    "            \n",
    "        \n",
    "        for i in range(AngleToDrone.shape[0]):\n",
    "            if timeStart(Distances[i])+400 < 10000:\n",
    "                \n",
    "                #The step that actually combines the IRs.\n",
    "                \n",
    "                Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400] = Total_IR[timeStart(Distances[i]):timeStart(Distances[i])+400]  + gen_IRs[i,:,0] * beam(AngleToDrone[i])*(1/(Distances[i]*Distances[i]))\n",
    "        \n",
    "        return Total_IR\n",
    "        \n",
    "        \n",
    "        \n",
    "# The sonar environment class.\n",
    "# Made to be similar to an OpenAI gym environment.\n",
    "\n",
    "class sonarEnv(core.Env):\n",
    "    \n",
    "    def __init__(self,ganWeights = '_2250',rotationAngle=45,speed=1,sepDist=3,dronesize=0.5):\n",
    "        \n",
    "        #Setup basic variables\n",
    "        super(sonarEnv, self).__init__()\n",
    "        \n",
    "        #4 actions (forward, back, left, right)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        #Observations are 10000 dimensional vector (the echo from the trees)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(10000,1), dtype=np.float32)\n",
    "        \n",
    "        #Radius of the drone\n",
    "        self.dronesize = dronesize\n",
    "        self.seed()\n",
    "        \n",
    "        #Load the GAN\n",
    "        self.gan = GenOnlyModel.ARGAN()\n",
    "        self.ganWeights = ganWeights\n",
    "        self.gan.load_weights(self.ganWeights)\n",
    "        \n",
    "        #How far trees are separated\n",
    "        self.sepDist = sepDist\n",
    "        \n",
    "        #Populate the world with some trees\n",
    "        self.generateInitalTrees()\n",
    "        \n",
    "        #Time\n",
    "        self.t = 0\n",
    "        \n",
    "        #Drone position\n",
    "        self.pos = np.array([0,5*self.sepDist,25])\n",
    "        \n",
    "        #Drone heading\n",
    "        self.heading=np.array([0,1,0])\n",
    "        \n",
    "        #Variables used to control the drone\n",
    "        self.done = False\n",
    "        self.r_left = np.eye(3)\n",
    "        self.r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(rotationAngle)),-np.sin(np.deg2rad(rotationAngle))],[np.sin(np.deg2rad(rotationAngle)),np.cos(np.deg2rad(rotationAngle))]])\n",
    "        self.r_right = np.eye(3)\n",
    "        self.r_right[0:2,0:2] = np.array([[np.cos(np.deg2rad(-rotationAngle)),-np.sin(np.deg2rad(-rotationAngle))],[np.sin(np.deg2rad(-rotationAngle)),np.cos(np.deg2rad(-rotationAngle))]])\n",
    "        self.speed=speed\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #This function moves the simulation forward by one timestep.\n",
    "        #The drone moves and a new observation is generated.\n",
    "        \n",
    "        self.t=self.t+1\n",
    "        \n",
    "        \n",
    "        #If we reach 10000 time steps reset\n",
    "        if self.t>10000:\n",
    "            self.done=True\n",
    "        \n",
    "        # forward -> 1 | left,right -> 0 | backwards -> -1\n",
    "        dirRew_dict = {\n",
    "            #direction(90deg) : reward\n",
    "            1 : 1,\n",
    "            0 : 0,\n",
    "            -1: -1 \n",
    "            }\n",
    "        \n",
    "        def update_reward():\n",
    "            reward = 0\n",
    "            if self.checkCollisions():\n",
    "                self.done=True\n",
    "                reward -= 3\n",
    "            elif int(self.heading[1]) == 0:\n",
    "                reward = dirRew_dict[int(self.heading[1])]*self.speed\n",
    "            else:\n",
    "                if action == 0:\n",
    "                    reward = dirRew_dict[int(self.heading[1])]*self.speed\n",
    "                else:\n",
    "                    reward = -dirRew_dict[int(self.heading[1])]*self.speed\n",
    "            return reward\n",
    "                \n",
    "        if action == 0:\n",
    "            #Forward\n",
    "            self.pos= self.pos + self.heading*self.speed\n",
    "            reward = update_reward()\n",
    "            self.state = self.getIR()\n",
    "        \n",
    "        elif action == 1:\n",
    "            #Back\n",
    "            self.pos=self.pos-self.heading*self.speed\n",
    "            reward = update_reward()\n",
    "            self.state = self.getIR()\n",
    "        elif action == 2:\n",
    "            #turn Left\n",
    "            self.heading = np.matmul(self.r_left,self.heading)\n",
    "            reward = int(self.heading[1])*0.25 - 0.25\n",
    "            self.state = self.getIR()\n",
    "        elif action == 3:\n",
    "            #turn Right\n",
    "            self.heading = np.matmul(self.r_right,self.heading)\n",
    "            reward = int(self.heading[1])*0.25 - 0.25\n",
    "            self.state = self.getIR()\n",
    "        \n",
    "        #This moves the trees and makes a new row if the ronde has moved forward enough\n",
    "        self.checkTreeRow()\n",
    "        \n",
    "        return np.array(self.state), reward, self.done, {}\n",
    "        #return np.array(self.state), reward, self.done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        #Reset the environment\n",
    "        self.t = 0\n",
    "        self.pos = np.array([5,5,25])\n",
    "        self.heading=np.array([0,1,0])\n",
    "        self.done = False\n",
    "        self.generateInitalTrees()\n",
    "        self.state = self.getIR()\n",
    "        \n",
    "        # self.gan.close()\n",
    "        # del self.gan\n",
    "        # keras.backend.clear_session()\n",
    "        # gc.collect()\n",
    "        # self.gan = ARGANmodel.ARGAN()\n",
    "        # self.gan.load_weights(self.ganWeights)\n",
    "        return self.state\n",
    "        \n",
    "    #The render functions all generate images, they have different properties and were ad hoc added as needed. Technically not necessary \n",
    "    def plot_results(self, avg_score_history,steps_arr, filename):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(steps_arr,  avg_score_history,'--', color='black')\n",
    "        ax.scatter(steps_arr,  avg_score_history, c=color_indices, cmap=colormap)\n",
    "        \n",
    "        \n",
    "        plt.plot(avg_score_history)\n",
    "        plt.title('Results '+filename, fontsize=14)\n",
    "        plt.xlabel('Num. Episodes', fontsize=14)\n",
    "        plt.ylabel('Avg. Score', fontsize=14)\n",
    "        plt.grid(True)\n",
    "        plt.savefig('outputs/ep_graph/'+filename+'_result.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def render(self,i,episode,reward):\n",
    "        figure, axis = plt.subplots(2, 1)\n",
    "        for t in self.TreeRow1:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lg = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lr = t.LeafPos[idx1]\n",
    "            axis[0].plot(lr[:,0],lr[:,1],'g.')\n",
    "            axis[0].plot(lg[:,0],lg[:,1],'r.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                axis[0].plot(t.center[0],t.center[1],'k*')\n",
    "            else:\n",
    "                axis[0].plot(t.center[0],t.center[1],'k*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            # print(\"Tree at Pos:%\"+str(t.pos))\n",
    "            # print(t.center)\n",
    "            # print(t.radius)\n",
    "            # print(t.maxx)\n",
    "            # print(t.maxy)\n",
    "            # print(t.minx)\n",
    "            # print(t.miny)\n",
    "            # idx = np.random.randint(0, t.LeafPos.shape[0], 3000)\n",
    "            DroneToLeaf = t.LeafPos-self.pos\n",
    "            Distances = np.linalg.norm(DroneToLeaf,axis=1)\n",
    "            idx = np.where(Distances < 4.3)\n",
    "            lr = t.LeafPos[idx]\n",
    "            idx1 = np.where(Distances > 4.3)\n",
    "            lg = t.LeafPos[idx1]\n",
    "            axis[0].plot(lg[:,0],lg[:,1],'g.')\n",
    "            axis[0].plot(lr[:,0],lr[:,1],'r.')\n",
    "            \n",
    "            if self.checkTreeDist(t):\n",
    "                axis[0].plot(t.center[0],t.center[1],'k*')\n",
    "            else:\n",
    "                axis[0].plot(t.center[0],t.center[1],'k*')\n",
    "            # circle1 = plt.Circle(t.center,t.radius,color='g',fill=False)\n",
    "            # plt.gcf().gca().add_artist(circle1)\n",
    "            \n",
    "        # North -> 1 |East/West -> 0 | South -> -1\n",
    "        compass_dict = {\n",
    "            #direction(90deg) : reward\n",
    "            1 : 'N',\n",
    "            0 : 'W/E',\n",
    "            -1: 'S'\n",
    "            }\n",
    "        \n",
    "        # For Simulated Enviroment\n",
    "        axis[0].plot(self.pos[0],self.pos[1],'b*', markersize=(self.dronesize*1))\n",
    "        axis[0].plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'b.', markersize=(self.dronesize*1 ))\n",
    "        axis[0].axis(xmin=self.pos[0]-10,xmax=self.pos[0]+10)\n",
    "        axis[0].axis(ymin=self.pos[1]-6,ymax=self.pos[1]+6)\n",
    "        axis[0].set_title(\"Simulated Env. & Agent Obs: Ep-\"+str(episode)+\" Step-\"+str(i) +\" Dir-\"+str(compass_dict[int(self.heading[1])])+\" Rew-\"+str(reward))\n",
    "        # For Cosine Function\n",
    "        axis[1].plot(self.state)\n",
    "        plt.savefig('outputs/states/Episode_'+str(episode)+\"_Step_\"+str(i)+\"_drone-size_\"+str(self.dronesize)+'.png',transparent=False)\n",
    "        plt.close()\n",
    "        \n",
    "        #plt.plot(self.state)\n",
    "        #plt.savefig('outputs/obs/'+str(self.dronesize)+'_step_'+str(i)+'_observedIR.png',transparent=False)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "    \n",
    "    def render_for_nips(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],'g.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state)\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()    \n",
    "    \n",
    "\n",
    "    def render_for_ave(self,i):\n",
    "        for t in self.TreeRow1:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        for t in self.TreeRow2:\n",
    "            \n",
    "            plt.plot(t.LeafPos[:,0],t.LeafPos[:,1],color=(232/256, 119/256, 34/256), linestyle='None', marker='.')\n",
    "            \n",
    "        plt.plot(self.pos[0],self.pos[1],'r*')\n",
    "        plt.plot(self.pos[0]+self.heading[0],self.pos[1]+self.heading[1],'r.')\n",
    "        plt.xlim([self.pos[0]-10,self.pos[0]+10])\n",
    "        plt.ylim([self.pos[1]-2,self.pos[1]+6])\n",
    "        \n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        \n",
    "        plt.savefig('outputs/states/'+str(self.dronesize)+'_'+str(i)+'.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        \n",
    "        plt.plot(self.state,color=(232/256, 119/256, 34/256))\n",
    "        plt.gca().axis('off')\n",
    "        \n",
    "        plt.gca().xaxis.set_ticklabels([])\n",
    "        plt.gca().yaxis.set_ticklabels([])\n",
    "        plt.savefig('outputs/obs/'+str(self.dronesize)+'_'+str(i)+'_observedIR.eps',transparent=True)\n",
    "        plt.show()\n",
    "        plt.cla() \n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.gan.close()\n",
    "        \n",
    "    def generateInitalTrees(self):\n",
    "        #Makes two rows of trees\n",
    "        \n",
    "        self.TreeRow1 = []\n",
    "        self.TreeRow2 = []\n",
    "        skip1 = np.random.choice(10)\n",
    "        skip2 = np.random.choice(10)\n",
    "        for i in range(10):\n",
    "            if i != skip1:\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            if i != skip2:\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "    def checkTreeRow(self):\n",
    "        #If you've gone past one row, make another row in front\n",
    "        \n",
    "        if self.pos[1] > 10+1:\n",
    "            self.pos[1] = self.pos[1] - 10\n",
    "            self.TreeRow1 = self.TreeRow2\n",
    "            for t in self.TreeRow1:\n",
    "                t.shift(np.array((0,-10,0)))\n",
    "            self.TreeRow2 = []\n",
    "            skip2 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip2:\n",
    "                    self.TreeRow2.append(Tree((i*3,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist-10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow2.append(Tree((i*self.sepDist+10*self.sepDist,10),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "    \n",
    "        if self.pos[1] < -1:\n",
    "            self.pos[1] = self.pos[1] +10\n",
    "            self.TreeRow2 = self.TreeRow1\n",
    "            for t in self.TreeRow2:\n",
    "                t.shift(np.array((0,10,0)))\n",
    "            self.TreeRow1 = []\n",
    "            skip1 = np.random.choice(10)\n",
    "            for i in range(10):\n",
    "                if i != skip1:\n",
    "                    self.TreeRow1.append(Tree((i*3,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist-10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "                    self.TreeRow1.append(Tree((i*self.sepDist+10*self.sepDist,0),np.random.choice(12)+1,np.random.uniform(0.0,90.0)))\n",
    "            \n",
    "                    \n",
    "        \n",
    "        if self.pos[0] < 0:\n",
    "            self.pos[0] = self.pos[0] +10*self.sepDist\n",
    "        if self.pos[0] > 10*self.sepDist:\n",
    "            self.pos[0] = self.pos[0] -10*self.sepDist\n",
    "    \n",
    "    def getIR(self):\n",
    "        \n",
    "        #Sum the IRs together from all the trees in range\n",
    "        Total_IR = np.zeros(10000)\n",
    "        for tree in self.TreeRow1:\n",
    "            if self.checkTreeDist(tree):\n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        for tree in self.TreeRow2:\n",
    "            if self.checkTreeDist(tree):\n",
    "                \n",
    "                Total_IR = Total_IR + tree.getEcho(self.pos,self.heading,self.gan)\n",
    "        \n",
    "        \n",
    "        return Total_IR\n",
    "    \n",
    "    def checkTreeDist(self,tree):\n",
    "        if np.linalg.norm((self.pos[0:2]-tree.center)) - tree.radius < 4.3:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def checkCollisions(self):\n",
    "        #Check if there is a collision with any trees\n",
    "        \n",
    "        for tree in self.TreeRow1:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        for tree in self.TreeRow2:\n",
    "            if self.pos[0] -self.dronesize/2< tree.maxx and self.pos[0]+self.dronesize/2 > tree.minx and self.pos[1]-self.dronesize/2 < tree.maxy and self.pos[1]+self.dronesize/2 > tree.miny:\n",
    "                if tree.checkCollision(self.pos,self.dronesize):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "# env = sonarEnv()\n",
    "# env.step(0)\n",
    "# env.step(0)\n",
    "# env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun 26 18:31:48 2020\n",
    "\n",
    "@author: sefun\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow.keras.layers import Dense,Activation, Input, concatenate, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches \n",
    "import numpy as np\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4, input_dims=128,load = False,\n",
    "                 fname_policy=None):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.action_Dict = {\n",
    "            0 : 'U',\n",
    "            1 : 'D',\n",
    "            2 : 'L',\n",
    "            3 : 'R'\n",
    "        }\n",
    "        self.stepDirct_history = [0,0,0,0]\n",
    "        self.steps_arr = [0]\n",
    "        self.score_arr = [0]\n",
    "        self.action_arr = ['U']\n",
    "        \n",
    "        self.load = load\n",
    "        self.model_file_policy = str(fname_policy)\n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        #load weights\n",
    "        if (self.model_file_policy != None and self.load == True):\n",
    "            self.policy.load_weights(self.model_file_policy)\n",
    "        \n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        #env2d = Input(shape=(self.input_dims,self.input_dims))\n",
    "\n",
    "        '''\n",
    "        7 layer dense sigmoid relu sigmoid \n",
    "        '''\n",
    "        #input layer\n",
    "        env1d = Input(name=\"state\",shape=(self.input_dims))\n",
    "        env = Flatten()(env1d)\n",
    "        #1st layer\n",
    "        denseSigmoid1 = Dense(64, activation=activations.sigmoid)(env)\n",
    "        dropout1 = Dropout(.3, input_shape=(64,))(denseSigmoid1)\n",
    "        #2nd layer\n",
    "        denseRelu2 = Dense(64, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(.3, input_shape=(64,))(denseRelu2)\n",
    "        #3rd layer\n",
    "        denseSigmoid3 = Dense(64, activation=activations.sigmoid)(dropout2)\n",
    "        dropout3 = Dropout(.3, input_shape=(64,))(denseSigmoid3)\n",
    "        #4th layer\n",
    "        denseRelu4 = Dense(32, activation='relu')(dropout3)\n",
    "        dropout4 = Dropout(.3, input_shape=(32,))(denseRelu4)\n",
    "        #5th layer\n",
    "        denseSigmoid5 = Dense(16, activation=activations.sigmoid)(dropout4)\n",
    "        dropout5 = Dropout(.3, input_shape=(16,))(denseSigmoid5)\n",
    "        #6th layer\n",
    "        denseRelu6 = Dense(16, activation='relu')(dropout5)\n",
    "        dropout6 = Dropout(.3, input_shape=(16,))(denseRelu6)\n",
    "        #7th layer\n",
    "        denseSigmoid7 = Dense(8, activation=activations.sigmoid)(dropout6)\n",
    "        #output layer\n",
    "        probs = Dense(self.n_actions, activation='softmax')(denseSigmoid7)\n",
    "       \n",
    "        \n",
    "        advantages = Input(name=\"advantage\",shape=[1])\n",
    "        def custom_loss(y_pred, y_true):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        policy = Model(inputs=[env1d,advantages], outputs=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        self.predict = Model(inputs=[env1d], outputs=[probs])\n",
    "        \n",
    "        return policy, self.predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        \n",
    "        probabilities = self.predict.predict(state)\n",
    "        action = np.random.choice(a = self.action_space, p=probabilities[0])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_transition(self, observation, action, reward, rewards):\n",
    "        state = observation\n",
    "        self.stepDirct_history[action] += 1\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(state)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "        self.steps_arr.append(steps)\n",
    "        self.score_arr.append(np.sum(rewards))\n",
    "        self.action_arr.append(self.action_Dict[action])\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "       \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G - mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory ,self.G], action_memory)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    def print_ep_stats(self, all_rewards):\n",
    "        print('ep: %6.1d' % i, ' score : %6.1f' % score,' steps : %-6.1f' % (steps-1),\n",
    "          'average_score : %5.2f' % np.mean(all_rewards[-10:]),\n",
    "          'F:%4.1f' % (self.stepDirct_history[0]/sum(self.stepDirct_history)),\n",
    "          'B:%4.1f' % (self.stepDirct_history[1]/sum(self.stepDirct_history)),\n",
    "          'L:%4.1f' % (self.stepDirct_history[2]/sum(self.stepDirct_history)),\n",
    "          'R:%4.1f' %(self.stepDirct_history[3]/sum(self.stepDirct_history)))\n",
    "    \n",
    "    def reset_ep_graph(self):\n",
    "        self.stepDirct_history = [0,0,0,0]\n",
    "        self.steps_arr = [0]\n",
    "        self.score_arr = [0]\n",
    "        self.action_arr = ['U']\n",
    "        \n",
    "    def plot_ep_graph(self, ep, all_rewards, stepDirct_history):\n",
    "        # Function to map the colors as a list from the input list of x variables\n",
    "        def pltcolor(lst):\n",
    "            cols=[]\n",
    "            for l in lst:\n",
    "                if l=='U':\n",
    "                    cols.append(0)\n",
    "                elif l=='L':\n",
    "                    cols.append(1)\n",
    "                elif l=='R':\n",
    "                    cols.append(2)\n",
    "                else:\n",
    "                    cols.append(3)\n",
    "            return cols\n",
    "    \n",
    "        colors = [\"green\", \"blue\", \"cyan\", \"red\"]\n",
    "        colormap = matplotlib.colors.ListedColormap(colors)\n",
    "        color_indices = pltcolor(self.action_arr)\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.steps_arr, self.score_arr,'--', color='black')\n",
    "        ax.scatter(self.steps_arr, self.score_arr, c=color_indices, cmap=colormap)\n",
    "        plt.title('Agent Score Vs Action Steps '+str(ep)+' avg_score:'+str(np.mean(all_rewards[-10:])), fontsize=14)\n",
    "        plt.xlabel('Num. Steps', fontsize=14)\n",
    "        plt.ylabel('Agent Score', fontsize=14)\n",
    "        \n",
    "        gre_patch = mpatches.Patch(color='green', label='Forward')\n",
    "        red_patch = mpatches.Patch(color='red', label='Back')\n",
    "        pin_patch = mpatches.Patch(color='cyan', label='Turn Right')\n",
    "        blu_patch = mpatches.Patch(color='blue', label='Turn Left')\n",
    "        \n",
    "        plt.legend(handles=[gre_patch, red_patch, blu_patch, pin_patch])\n",
    "        plt.grid(True)\n",
    "        plt.savefig('outputs/ep_graph/ep_0_rot'+str(ep)+'.png',transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        #direction count\n",
    "        plt.bar([1,2,3,4], height = self.stepDirct_history)\n",
    "        plt.xticks([1,2,3,4], ['Forwards','backwards','T Left', 'T Right'])\n",
    "        plt.savefig('outputs/ep_graph/Action_Prob_Dist_'+str(i)+'.png',transparent=False)\n",
    "        plt.close()\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save_weights(self.model_file_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-415565e36b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Vanilla Actor critque\n",
    "Created on Sat Jan 15 23:45:00 2022\n",
    "\n",
    "@author: sefun\n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "n_steps = 30\n",
    "n_episodes = 3000\n",
    "num_inputs = 10000\n",
    "num_actions = 4\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "filename = \"a2c_7L_Weights_final\"\n",
    "agent = Agent(ALPHA=learning_rate, GAMMA=gamma, input_dims=num_inputs, n_actions=num_actions,load = True, fname_policy=str(\"models/\"+filename+\"_policy.h5\"))\n",
    "\n",
    "actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "env = ENV.sonarEnv(rotationAngle=90)\n",
    "\n",
    "#steps record\n",
    "all_lengths = []\n",
    "average_lengths = []\n",
    "steps_history = []\n",
    "#step direction record\n",
    "stepDirct_history = [0,0,0,0]\n",
    "action_Dict = {\n",
    "    0 : 'U',\n",
    "    1 : 'D',\n",
    "    2 : 'L',\n",
    "    3 : 'R'\n",
    "}\n",
    "#reward record\n",
    "all_rewards = []\n",
    "score_history = []\n",
    "avg_score_history = []\n",
    "\n",
    "entropy_term = 0\n",
    "\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    strAction = ''\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    steps_arr = [0]\n",
    "    score_arr = [0]\n",
    "    action_arr = ['U']\n",
    "    #start = time.time() #4sec-5steps 11sec-20steps 21sec-30steps\n",
    "    while not done and steps < n_steps:\n",
    "        #select action\n",
    "        action = agent.choose_action(state)\n",
    "        #record direction\n",
    "        stepDirct_history[action] += 1\n",
    "        #step\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #store transitions: state, action, reward, self.log_prob in self.log_probs, self.log_prob in self.log_probs\n",
    "        agent.store_transition(state,action, reward)\n",
    "        state = new_state\n",
    "        rewards.append(reward)\n",
    "        score = np.sum(rewards)\n",
    "        steps += 1\n",
    "        steps_arr.append(steps)\n",
    "        score_arr.append(np.sum(rewards))\n",
    "        action_arr.append(action_Dict[action])\n",
    "        if(i%500 == 0):\n",
    "            env.render(steps,i,reward)\n",
    "            \n",
    "    score_history.append(np.sum(rewards))\n",
    "    steps_history.append(steps)\n",
    "    agent.learn()\n",
    "    if(i%10== 0):\n",
    "        agent.ep_graph(steps_arr,score_arr,action_arr,i, np.mean(score_history[-10:]))\n",
    "        plt.bar([1,2,3,4], height = stepDirct_history)\n",
    "        plt.xticks([1,2,3,4], ['Forwards','backwards','T Left', 'T Right'])\n",
    "        plt.savefig('outputs/ep_graph/Action_Prob_Dist_'+str(i)+'.png',transparent=False)\n",
    "    print('ep: %6.1d' % i, ' score : %6.1f' % score,' steps : %-6.1f' % (steps-1),\n",
    "          'average_score : %5.2f' % np.mean(score_history[-10:]),\n",
    "          'F:%4.1f' % (stepDirct_history[0]/sum(stepDirct_history)),\n",
    "          'B:%4.1f' % (stepDirct_history[1]/sum(stepDirct_history)),\n",
    "          'L:%4.1f' % (stepDirct_history[2]/sum(stepDirct_history)),\n",
    "          'R:%4.1f' %(stepDirct_history[3]/sum(stepDirct_history)))\n",
    "    \n",
    "    stepDirct_history = [0,0,0,0] \n",
    "    avg_score_history.append(np.mean(score_history[-10:]))\n",
    "    if (i%500 == 0):\n",
    "        agent.save_model()\n",
    "        \n",
    "\n",
    "# transfer function to agnent class\n",
    "env.plot_results(avg_score_history, steps_arr,filename)\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0f3e08f09915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActorCritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c(env):\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_outputs = env.action_space.n\n",
    "    \n",
    "    actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "    ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    entropy_term = 0\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for steps in range(num_steps):\n",
    "            value, policy_dist = actor_critic.forward(state)\n",
    "            value = value.detach().numpy()[0,0]\n",
    "            dist = policy_dist.detach().numpy() \n",
    "\n",
    "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "            log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term += entropy\n",
    "            state = new_state\n",
    "            \n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _ = actor_critic.forward(new_state)\n",
    "                Qval = Qval.detach().numpy()[0,0]\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                if episode % 10 == 0:                    \n",
    "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        Qvals = np.zeros_like(values)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "            Qvals[t] = Qval\n",
    "  \n",
    "        #update actor critic\n",
    "        values = torch.FloatTensor(values)\n",
    "        Qvals = torch.FloatTensor(Qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "        ac_optimizer.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        ac_optimizer.step()\n",
    "\n",
    "        \n",
    "    \n",
    "    # Plot results\n",
    "    smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "    smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "    plt.plot(all_rewards)\n",
    "    plt.plot(smoothend_rewards)\n",
    "    plt.plot()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(all_lengths)\n",
    "    plt.plot(average_lengths)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-89d06ebb74c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mheading\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.plot'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Query - heading  reward plot\n",
    "rotaion angle 90\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.plot as plt\n",
    "\n",
    "heading = np.array([0,1,0])\n",
    "rotationAngle = 90\n",
    "\n",
    "r_left = np.eye(3)\n",
    "r_left[0:2,0:2] = np.array([[np.cos(np.deg2rad(rotationAngle)),-np.sin(np.deg2rad(rotationAngle))],[np.sin(np.deg2rad(rotationAngle)),np.cos(np.deg2rad(rotationAngle))]])\n",
    "r_right = np.eye(3)\n",
    "r_right[0:2,0:2] = np.array([[np.cos(np.deg2rad(-rotationAngle)),-np.sin(np.deg2rad(-rotationAngle))],[np.sin(np.deg2rad(-rotationAngle)),np.cos(np.deg2rad(-rotationAngle))]])\n",
    "  \n",
    "\n",
    "#\n",
    "#heading = np.matmul(r_right,heading)\n",
    "y = list()\n",
    "x = list()\n",
    "\n",
    "# forward -> 1 | left,right -> 0 | backwards -> -1\n",
    "dirRew_dict = {\n",
    "        #direction(90deg) : reward\n",
    "        1 : 2,\n",
    "        -1 : -2,\n",
    "        0 : -1\n",
    "    }\n",
    "\n",
    "for i in range(9):\n",
    "    y.append(dirRew_dict[int(heading[1])])\n",
    "    x.append(i)\n",
    "    heading = np.matmul(r_left,heading)\n",
    "    \n",
    "plt.plot(x, y);\n",
    "print(heading)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEbCAYAAADZFj8oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9OklEQVR4nO3dd3hU1dbA4d9KI5UOQQWliQIh9H7pgihgpemlKVfs4udFriD2ci2INAEpAlIuIgii2IEICKiAlBAsICCdhJ6EhJDs748zhCGmTMIkJzOz3ueZJ5mzT1lzZmbNnn327C3GGJRSSnk/P7sDUEopVTQ04SullI/QhK+UUj5CE75SSvkITfhKKeUjNOErpZSP0ISvlBcQESMiPe2OQxVvmvA9lIg0FJF0EfnB5jhiRGSiC+tVEJFJIrJXRFJF5KiIrBCRzkURp6tEJFJE0kSkXw7lb4nIfhEp8HtHRCqKSIqI/JXf/YjILBH5PJuiq4DPChpTPo5/h4isF5FTIpIoIr+KyHSn8kEikljYcaiC0YTvuR4AJgFRIlLb7mBcsBhoBgwGagHdgS+BcoV1QBEJyu82xpijwOdYcWbdXwDQH5hpjMm4gtAGYSXnFODmK9hPJmPMEWNMqjv2lRMR6QR8jBV7C6Ah8DQghXlc5UbGGL152A0IAU4B0cAMYHQ26zQHNmMllV+AWwEDtHdapw6wHDgLHAP+B1RyKp+FlfyGAgeBk8BMINSp3GS5Vc0mltKOspvyeFxBwOvAPiAV+BN4wqm8LfCj4zEdBd4FgpzKY4DJwGggHvjZlceZTRzdgAygepbltzuWV3XcrwJ8CpwAkoFfgb4uPH+/Aj2A54BF2ZTfCCwDTgOJwHqgHvBiNue7vWMbA/R02kc94DvgnCO+WUApV5/bHOIeC6zNpbx9NvG96PTcvgkcAJKAn4Gbs9m2O7DF8RxvAho7rVMKmON4DlMcr48n7X4/etLN9gD0VoAnzaplbnX8397xBgh0Kg93JLz5QF2gM7AjS4K4CkhwvAlrY314fAb8BPg51pnlSDrTHOt0wfqgGeEoLwWsAz4AKjlu/tnEG4CVbMcDwbk8rv85EsLdQHWgAzDAUXaNI1FMccTSHTgCvOO0fYzjOO9gJc3arjzObOLwd8TxSpbly4Bvne5/BnwL1AeqAV2Brnk8d20cz02gY5tUoIJT+dWOeD/F+kZUC+gHNHA8rx85jnnxfAc5tstM+EAoVhJfipX42wG/A4udjpPrc5tD7M84YqufQ3kQ1gdIklN84Y6yecAGrA/t6sBjwPmL++JSwv8V61tPFNa3iSNcqmBMwPowaAZUdWzTy+73oyfdbA9AbwV40uB7YJjjfwH2Anc7lT+IVasLcVp2L5cn/JeBFVn2W8axTjPH/VnAfiDAaZ1pwHdO92OAiS7EfLcjphSsGutooLlT+fWOY2ebMIHXgF04JWmsppFUp4QQA2zLsl2ejzOH473qeOwXP/wqAWlAH6d1tgEv5PO5m+18voDVwL+zPM59OH1zybL9LODzbJY7J/wHsJJ5hFP5xYRa09XnNptjhGF9UzKObRcBD+FI6k7PSWKW7WpgfTO6NsvypcCkLPH906k8HOtD6F+O+8uwmtNsfw966k3b8D2MiNQEWmPV3jHWO2Ee8C+n1W4EYo0x55yW/ZhlV42Bto4Lb4mOC237HWU1nNaLM8ZccLp/CKiY37iNMYuxaq89sNruWwEbRGSkY5WGWElhVQ67qA2sN5e3na/FqlXWdFq2Kct2rj7OrD7A+lbRxXF/IFYSXeq0zjhglOMi5qsi0jiX/SEiJYGeWM0SF83h8usFDbGaTc7ntq881Mb64DvrtGwd1vmt47QsX8+tMSbJGNMN63y/hJWM/wvsEJHIXOJphFUxicvyPHTj78/BeqfjJQLbnWKeDPQWka0iMlpE2uVyTJWNALsDUPn2L6wmh79EMq+VCYCIVDHG7HfcN3nsxw+rtjYsm7KjTv+nZSkzFPBivzEmBas54lvgZUfvjhdFZDR5X/jL7TE5L0/KUubq48wa658isgq4H/jK8XeucbowaoyZISJfY10fuQlYJyL/Nca8mMNu78VqbvnB6bkD8BeR1saYH3DPBVBXz1WBnltjzG5gNzBdRF7Dai56GOsaQ3b8HPtums0xz/199RyP+6WIXAfcAnQClovIx8aY+1zdh6/TGr4HcfQSGQiMwGrTvXirj9W8cPGFvxOoJyIhTps3y7K7zVjt+/uMMbuy3M7iuvNYH0AFEYdV6Qh2xOOH1W6f07ots3Rj/Ifj+LtzOcaVPM7pwO0icidWW/r0rCsYYw4YY6YaY3oDzwNDctnfYGAilz93DbA+kC7W8jcD/8ilh5Er5zsOqC8iEU7LWmGd3515bJtfe7EuWIfnEt8vWB9ClbJ5Dg5mWbfFxX9EJAyrLT8zZmNMgjFmjjFmENY5GygiJdz5gLya3W1KenP9htVLJA0ol03Zf7DefH5cumg7F+vr8E1YX40N0M6x/tVYF3s/werRU92x3lQcbb9k016MVYuLdbo/FasZpSpQnmwuhGJ1vVyJdfExGutiZS+sC3LOF0E/4tJF22pYFzj7O8qyXrTtRvYXbSdmOXaejzOX810COI517eHHbMrHYV2orY6VuFeRQxu443EbIDqbsj5YvXEiHI/zOFbTUVOs5pN7gAaOdUdiNUnd4DjfgY7lWS/aHgKWYF20bQv8xt8v2ub63GYT54vAW1jt7dWwmp9mAulAW8c6rRyxdHbEd/H6ylysaxM9HeerCda3rrsc5e0d28U5tq3reD0cBcIc67wM3IF1vae2o3yX3e9LT7rZHoDe8vFkWRetvsmhrLrjDdPFcb8FVs0q1fH3bkd51guli7C65J1zJIUJXOr5kWdSwKr5rseq5Rmy75ZZAqu75c+OYyUDfwBjgLJZ1nsLq4dJKlbN/TGn8ovdMlO51C2zhFN5DNlcQM7rceZxzsc7HtcD2ZRNcDyOFKwP2AXANbns5/ccysIc52SI435d4AusD4GzWO3vUY6yCsA3juXOF+EzE77jfj1ghePxniSHbpm5PbfZxNkBq+fMPsdjPob1Idcty3qTsXrzGC51ywx07P9PrG8BR7Bez40d5e0d69+G9W01FevbTlOn/T6L1dssGetD+Augtt3vS0+6ieNEKi8nIrdj1fgqGmMS7I5HKWci0h7rw6OCvj4Lj1609VIiMhCrNrUfqx10LPCZvpmU8l2a8L1XJFbXuauwvj4vx2rnV0r5KG3SUUopH6HdMpVSykcU6yad8uXLm6pVqxZo26SkJMLCwtwbkIfSc3E5PR+X0/NxiTeci02bNiUYYypkV1asE37VqlXZuHFjgbaNiYmhffv27g3IQ+m5uJyej8vp+bjEG86FiOzLqUybdJRSykdowldKKR+hCV8ppXxEsW7Dz05aWhoHDhwgJSUl1/VKlSrFzp3uHifKswQHB1O5cmW7w1BKFRMel/APHDhAREQEVatWJcsQs5c5e/YsEREROZZ7O2MMx48f58CBA3aHopRykTGG7ce2czb1LI2vbkxwQLBb9+9xCT8lJSXPZK9ARChXrhzx8fF2h6KUcsEfx/+g2/xuHDp7CH8/f4wxTOk+hXvr3eu2Y3hkG74me9foeVLKM2SYDG768CZ2ndhFUloSZ1LPcPb8WR5Y9gDbj25323E8MuErpZQ3WbNvDSdTTmIw1qDTDqnpqUzeONltx/G4Jp2sKo2uxNGkHGeqy7fIsEiODDuS6zr+/v7Uq1cv8/7SpUsp6C+C3aV9+/aMHj2aJk2a2BqHUir/EpITMKnGGuH/Z6AvcCOkm3QOnz3stuN4fMJ3Z7J3dX8hISFs2bIl3/u+cOECAQFXfsrdtR+lVPGQ8msKiWMT4TTWvGzVrOVhgWH0uKGH246jTTpusmXLFlq0aEF0dDR33nknJ0+eBKya98iRI2nXrh3jxo2jevXqGGM4deoUfn5+rF69GoA2bdqwa9cufvrpJ1q1akXDhg1p1aoVv/32GwCzZs2iV69e9OjRgy5dunDu3Dn69u1LdHQ0ffr04dw5l+eCVkoVI//73//od3c/ypcqT/CDwdYU7SUgJCCE6mWqu/WirVYTC+DcuXM0aNAAgGrVqrFkyRIGDBjAhAkTaNeuHc8//zwvvfQSY8eOBeDUqVN8//33AHz77bfExcWxZ88eGjduzJo1a2jevDkHDhygZs2anDlzhtWrVxMQEMB3333HyJEjWbx4MQDr169n27ZtlC1bljFjxhAaGsq2bdvYtm0bjRo1suNUKKUK4GK36fLly3P77bfz9ttv8/jjj/Ptvm9576f3OJV6it51ejOk8RC3ds3UhF8AWZt0Tp8+zalTp2jXrh0AAwcOpFevXpnlffr0yfy/TZs2rF69mj179jBixAimTZtGu3btaNq0aea+Bg4cyB9//IGIkJaWlrlt586dKVu2LACrV6/miSeeACA6Opro6OhCe7xKKfc5fPgwjzzyCNu3b2fbtm2EhoYybNgwALrX6k73Wt0L7djapFMEnIdbbdOmDWvWrOGnn37i1ltv5dSpU8TExNC2bVsAnnvuOTp06EBsbCyfffbZZb8ozjpsq3a7VMpzGGP44IMPqF27Nl999RUPPvggQUFBRRqDJnw3KFWqFGXKlGHNmjUAzJkzJ7O2n1Xz5s1Zt24dfn5+BAcH06BBA95//33atGkDWDX8a665BrDa7XPStm1b5s2bB0BsbCzbtm1z4yNSSrnTyZMn6dKlC4MHD6Z+/fps3bqVp59+usg7X3h8wo8MiywW+5s9ezZPP/000dHRbNmyheeffz7b9UqUKEGVKlVo0aIFYNX4z549m9nNc/jw4YwYMYLWrVuTnp6e4/EefvhhEhMTiY6O5q233qJZs2YFilspVfhKlixJQEAAkydPZtWqVdSqVcuWOIr1nLZNmjQxWSdA2blzJ7Vr185zW18fS+einTt3cvToUY+f1MGdvGGSC3fS83GJO89FXFwcw4cP54MPPqBixYoYY4qkGVZENhljsv1BjsfX8JVSqjg5f/48r776Kg0bNmTDhg38+uuvQPG45qYJXyml3GTjxo00bdqU5557jrvuuou4uLjMDhnFgXbLVEopNxkzZgwJCQl8+umn3HbbbXaH8zea8JVS6gp8//33REZGcuONNzJhwgT8/f0pXbq03WFlS5t0lFKqAM6cOcPDDz9M+/bteemllwAoV65csU32oAlfKaXy7YsvvqBu3bpMnTqVp556iunTp9sdkks8P+FXqgQif7tFlCyZ7fI8b5Uq5XlIf39/GjRoQP369WnUqBHr1q0rUOiDBg1i0aJFBdpWKWWP//3vf3Tr1o1SpUqxbt063nnnnb/9Cr648vw2/KPuHR7Zlf05j6Xz9ddfM2LEiMzB0ZRS3scYQ0JCAhUqVOD222/nnXfe4bHHHivyoRGulOfX8G125swZypQpA0BiYiKdOnWiUaNG1KtXj08//TRzvQ8//JDo6Gjq169P//79/7af5557jkGDBpGRkVFksSul8nbw4EHuuOMOWrZsSXJyMqGhoTz11FMel+zBG2r4Nrg4PHJKSgqHDx9m5cqVAAQHB7NkyRJKlixJQkICLVq04LbbbiMuLo7XXnuNH374gfLly3PixInL9jd8+HBOnz7NzJkzi8WPM5TyRbHHYjmTeoZjSceoGGb9Mnb69OkMGzaMtLQ0XnnlFY9M8s6KNOGLyP8B/wIMsB24zxiTkvtWxY9zk8769esZMGAAsbGxGGMYOXIkq1evxs/Pj4MHD3L06FFWrlxJz549KV++PEDmEMcAr7zyCs2bN2fq1Kl2PBSlfF5CcgK3zruVHfE7eK36a/R5tw+Daw8m7r04Vq1aRfv27Zk2bRo1a9a0O9QrVmRNOiJyDfAE0MQYEwX4Y83c6NFatmxJQkIC8fHxzJs3j/j4eDZt2sSWLVuIjIwkJSUl1zE0mjZtyqZNm/5W61dKFY17Ft3DliNbSE5LJt2kk5KewsydMzmRdoKpU6eycuVKr0j2UPRt+AFAiIgEAKHAoSI+vtv9+uuvpKenU65cOU6fPk3FihUJDAxk1apV7Nu3D4BOnTqxcOFCjh8/DnBZcu/atSvPPPMM3bp14+zZs7Y8BqV8VXxSPGv+WkNaRhochWlvToNESE5PRv4pPPDAA17VzFpkTTrGmIMiMhr4CzgHfGOM+SbreiIyBBgCEBkZSUxMzGXlpUqVuiwxhlWsiN+xY26LM6NiRZLySLznzp3LnGHKGMPkyZNJTk7m9ttvp3fv3pkXbWvVqkViYiLXXXcdTz31FG3atMHf35/o6GimTJlCWloa586d44477iA+Pp5u3bqxaNEiQkJC3PZ4UlJSSExM/Nt59GV6Pi7ny+cjNT2VV6q+wndLv2PF0hUcCjvEI6GPUL1WdYL8g7zvvBhjiuQGlAFWAhWAQGAp0C+3bRo3bmyyiouL+9uy7Jw5c8al9bxdXFycWbVqld1hFCt6Pi7ny+dj3fp1xr+SvwEM9TAvTXnJ8CIm8OVA89jyx+wOr0CAjSaHnFqUTTo3AXuMMfHGmDTgE6BVER5fKaUuM2H8BEqb0pToXwL/nv6ElQwjOCCYcqHleLbts3aH53ZF2UvnL6CFiIRiNel0AjbmvolSSrnXqlWrqFSpErVr12bChAkEBASwL2UfYzeMJeJCBM+2eZaHmzxMudBydofqdkVWwzfG/AgsAjZjdcn0A7QvolKqSJw+fZoHH3yQjh078sorrwDWYGelSpUiOjKaD27/gFrlajGq7SivTPZQxP3wjTEvAC8U5TGVUmrZsmU8/PDDHDlyhGHDhmWObulr9Je2SimvNm/ePPr160e9evVYunQpTZs2tTsk2+hYOkopr2OM4ahjIMS77rqLd999N3P6QV/m8Qk/h9GRKVkyorBGRyY8PNzl+OLj42nevDkNGzZkzZo1TJo06QoerVIqL/v376dHjx60atWK5ORkQkJCePLJJz1+HBx38PiEb8PoyPmyYsUKbrzxRn755ReqVKmiCV+pQpKRkcH7779P3bp1WbVqFU888QQlSpSwO6xiRdvw3WT37t08+uijxMfHExoayrRp00hJSWH48OGZo2vecMMN7N69mwYNGtC5c2fefvttu8NWyiucOHGCu+66i++//55OnToxdepUqlevbndYxY4mfDcZMmQIU6ZM4frrr+fHH3/kkUceYeXKlbz88sts3LiRiRMnsnfvXnbs2JE50qZSyj1Kly5NREQE06dP5/777/eq8W/cSRO+GyQmJrJu3Tp69eqVuSw1NdXGiJTyftu2bePpp59m9uzZVKpUic8++8zukIo9TfhukJGRQenSpbXmrlQRSE1N5fXXX+f111+nTJky7Nq1i0qu9LZQnn/RtjgoWbIk1apV4+OPPwasLmFbt27923oRERE6BLJSV2DDhg00atSIl19+mb59+7Jz507+8Y9/2B2Wx/D4hB8ZWfT7S05OpnLlypm3MWPGMG/ePGbMmEH9+vWpW7fuZfPZXlSuXDlat25NVFQUTz/9tHsDV8oHjB8/nrNnz7J8+XLmzJlDuXLeOQRCYfH4Jp0jR7JffvbsWSIiIgrlmDlNNP7VV1/9bdmgQYMYNGhQ5v358+cXSkxKeasVK1Zw1VVXUadOHSZOnEhAQAAlS5a0OyyP5PE1fKWUdzp16hT/+te/uOmmm3j11VcBaz5oTfYFpwlfKWWr3xJ+Y8nOJcTFx2UuW7p0KXXq1GHWrFk888wzzJgxw8YIvYfHN+kopTxTyoUUei7syco9Kwn0DyQtPY1WVVpxr7mXwYMGU79+fT777DMaN25sd6heQxO+UsoWo1aOYsWeFaRcSOFc2jlIhLVmLVVrV2X8+PE89NBDBAYG2h2mV9GEr5SyxfTN00m5kAKngM+BBEh9JJX5v85n2shp+mvZQqAJXylli+TzyfAT8B1gsGa9DoDU9FQMBkETvrt5fsJftwXSLvxtcYE7ZAYGQKsGua7i7+9PvXr1uHDhAtWqVWPOnDmULl2aQ4cO8cQTT7Bo0aJctw8PDycxMfFvy5cuXUqtWrWoU6dOQaNXyiOcOHGCsPlhnPrtFFQHegBlrLKWlVviJ9qfpDB4/lnNJtkX9v5CQkLYsmULsbGxlC1blvfeew+Aq6++Os9kn5ulS5cSFxeX94pKebjSpUvTqFojQnqGEDQoCMpACf8SRARFMKmbDiFeWDw/4dusZcuWHDx4EIC9e/cSFRUFWL/G7d27N9HR0fTp04fmzZuzcePGzO2effZZ6tevT4sWLTh69Cjr1q1j2bJlPP300zRo0IDdu3fb8niUKixbtmzhpptu4siRI/j5+bHiyxX8OfNPnvnHM9xa81aeavkUOx/dSXRktN2hei3Pb9KxUXp6OitWrGDw4MF/K5s0aRJlypRh27ZtxMbG0qBBg8yypKQkWrRowWuvvcbw4cOZNm0ao0aN4rbbbqN79+707NmzCB+FUoUrJSWFV155hTfffJPy5cuze/fuzMHOKoVX4qUOvjmhuB20hl8AFyc0KVeuHCdOnKBz585/W2ft2rX07dsXgKioKKKjL9VagoKC6N69OwCNGzdm7969RRK3UkXthx9+oEGDBrz++uv079+fuLg4WrdubXdYPksTfgFcbMPft28f58+fz2zDd2aMyXH7wMDAzC5n/v7+XLjg5usQShUTkyZNIiUlha+//pqZM2dStmxZu0PyaZrwr0CpUqUYP348o0ePJi0t7bKyf/zjHyxcuBCAuLg4tm/fnuf+dPhk5Q2++eabzM4HEydOJDY2li5dutgclQJvSPiBbr4Mkc/9NWzYkPr167NgwYLLlj/yyCPEx8cTHR3Nm2++SXR0NKVKlcp1X3379uXtt9+mYcOGetFWeZyTJ09y3333cfPNN/P6668DUKZMGcLDw22OTF3k+Rdtc+gzX5jDI2ftQ+88tVpsbCwAwcHBzJ07l+DgYHbv3k2nTp247rrr/rZ9z549My/Stm7dWrtlKo/0ySef8OijjxIfH8/IkSN57rnn7A5JZcPzE34xlZycTIcOHUhLS8MYw+TJkwkKCrI7LKXcbu7cufTv35+GDRvy5ZdfXtYjTRUvmvALSURExGX97pXyJsYYjhw5wlVXXcXdd9/N6dOnGTJkiA52Vsx5ZBt+bj1g1CV6nlRh2Lt3LzfffDOtW7cmKSmJkJAQHn30UU32HsDjEn5wcDDHjx/XZJYHYwzHjx8nODjY7lCUl8jIyGDChAlERUWxfv16hg0bRkhIiN1hqXzIV5OOiDQBagCfG2OSRCQMSDXGFFlH8sqVK3PgwAHi4+NzXS8lJcXnk11wcDCVK1dm3759doeiPNyJEyfo0aMH69ato2vXrkyZMiWzE4LyHC4lfBGJBJYBTbEGMr0e+BMYA6QAQwsrwKwCAwOpVq1anuvFxMTQsGHDIohIKe9XunRpIiMj+fDDD+nXr5+OVe+hXG3SeRc4ApQDkp2WfwzoLyqU8kKbN2+mY8eOHD58GD8/Pz755BP69++vyd6DuZrwOwHPGmNOZlm+G7jWvSEppex07tw5RowYQbNmzdi5cyd79uyxOyTlJq624YcA57NZXgGrSUcp5WH2nNzDhgMbCD8fTobJwE/8WLt2LYMHD+b333/n/vvvZ/To0ZQpU8buUJWbuFrDXw0McrpvRMQf+A+wwtWDiUhpEVkkIr+KyE4Rael6qEopdzDG8NDnD1FnUh0e/PxBdp3YRfVx1dl7ai9Tpkzh/PnzfPvtt8yYMUOTvZdxtYY/HPheRJoCJYB3gLpAKSA/Y52OA74yxvQUkSAgND/BKqWu3Lzt85i7bS4pF1JIIYUdv+zgr8S/uHPBnaycsJLAwEAd/8ZLuZTwjTFxIlIPeBhIBYKxLti+Z4w57Mo+RKQk0BbHNwVjzHmybyZSShWiiT9NJCktyep+8RXM2DYD6sFvV//GKU5RLTzvXnDKM0leP2ASkUBgLTDAGPNbgQ8k0gCYCsQB9YFNwFBjTFKW9YYAQwAiIyMbZx2F0lWJiYlaS3HQc3E5Xz8fO47tYMMPG1gyawnnks5xx5130KxHM4KCgqhdvjbBAb77+xVveG106NBhkzGmSbaFxpg8b8AxoJYr6+ayjybABaC54/444JXctmncuLEpqFWrVhV4W2+j5+Jyvn4+7hp5lwEMV2F4CDN6/mjDi5gKb1UwF9Iv2B2erbzhtQFsNDnkVFcv2s4GHriijx04ABwwxvzouL8IaHSF+1RKucAYw8GDBwGYPHwykX0iCX04FCqBIIQGhjL7jtn4+/nbHKkqTK5etA0D/ikinbGaYi5rhjHGPJHXDowxR0Rkv4jcYKymoU5YzTtKqUK0Z88ehgwZwq5du4iNjaViqYrsmbOH+dvn882f31CxREW2PrSVmmVr2h2qKmSuJvzawGbH/9WzlOVnFLPHgXmOHjp/AvflY1ulVD6kp6czceJERo4cib+/P2+99VbmYGchgSEMbjSYwY0GExMTo8neR7jaS6eDOw5mjNmC1ZavlCpEx48fp3v37mzYsIFbb72VKVOmUKVKFbvDUjbL72iZwUBNrFr9bmOM/spWqWKoTJkyXH311cydO5d7771Xx79RgIu/tBWRQBF5GzgJbAW2AydF5C1Ht02llM1+/vln2rVrx6FDh/Dz82Px4sX885//1GSvMrnaS+dNoB/wEFALa3jkh4H+wH8LJzSllCuSk5MZPnw4LVq0YNeuXfz11192h6SKKVebdO4F7jfGfOG0bLeIxAPTgWFuj0wplaeYmBgeeOABdu3axQMPPMDbb79NqVKl7A5LFVOuJvxSWEMhZ7UbKO22aJRS+TJjxgwyMjJYsWIFHTt2tDscVcy5mvC3Ak8Aj2ZZPhTY4s6AlFK5W758Oddddx1RUVFMmDCBwMBAwsLC7A5LeQBX2/CHAwNF5HcRmS0is0TkN6x2/acLLzyl1EUJCQn069eP7t2789ZbbwHW1IOa7JWrXEr4xpjVwA1YI2SGAyUd/99gjFlbeOEppYwxLFiwgNq1a7Nw4UJeeOEFpk+fbndYygO53A/fGHMQeLYQY1FKZWPOnDkMHDiQpk2bMmPGDOrVq2d3SMpDuZTwReQx4JQxZm6W5f2AksaYSYURnFK+KiMjg4MHD1KlShV69+5NSkoKgwcPxt9fBzdTBedqG/6TwP5slu8F/s9dwSilYNeuXXTq1Ik2bdqQlJREcHAwQ4YM0WSvrpirCb8ysC+b5QccZUqpK5Sens4777xDdHQ0mzdvZtSoUYSG6iygyn1cbcM/AjTAqtE7awQkuDEepbze4bOH+Wb3N4QEhtDt+m6EBYVx/PhxbrnlFn7++Wd69OjB5MmTueaaa+wOVXkZVxP+fGC8iCQBMY5lHYCxwDz3h6WUdxq9bjTPrXqOAAlARDAYPu37Ke2va0/VqlV56qmn6NOnj45/owqFqwn/BaAa8DWQ7ljmh9U187lCiEspr7Pp0CZeiHmBlAuOQWYPAN9Aj9M9iH8lnoULF9oan/J+rvbDTzPG3IM1cNq9wD+x+uD3NcakFWaASnmLWVtmWcn+PFbVaQbW+LOn4etdX9sbnPIJ+RoP3xizC9glIgGA705tr1QBJKclk/FnBizDSvSNgc7gX9KfcxfO2Ryd8gW51vBFpJOI9M6y7BkgETglIl+JSOlCjE8pr9GzTk8CtgaAAIOAHkAwnE8/T+fqne0NTvmEvJp0nsGp26WINANeB+Zgja9TH/31rVK5WrZsGdu3b6drza50f7I7oY+HQlXwF39CAkJ4p8s7VAirYHeYygfk1aRTDyvpX9QLWGeMeQBARPYDr6IDqCn1N8eOHeOJJ57go48+on///nz44Yd8MuATvv3zWxbvXEx4UDiD6g+iXqQOlaCKRl4JvzRwzOl+a8B5EpSfAe0srJQTYwzz589n6NChnD17lldeeYXhw4cDICJ0qdGFLjW62Byl8kV5NekcBmoAiEgJoCGw3qk8AkgtnNCU8kwffvgh/fr14/rrr+eXX35h1KhRBAUF2R2WUnnW8L8E3nJcqL0NSALWOJVHA7sKKTalPEZGRgYHDhzg2muvpU+fPly4cIFBgwbp+DeqWMmrhv88kAJ8B9wPPGCMOe9Ufj/wbSHFppRH+OOPP+jYseNlg53pyJaqOMq1hm+MSQDaikgpINEYk55llV5YXTSV8jkXLlzg3Xff5fnnn6dEiRKMGTNGBztTxZpLP7wyxpzOYfkJ94ajlGdISEjglltuYePGjdxxxx289957XH311XaHpVSuXB0eWSnlpGzZstSoUYOFCxfyySefaLJXHkETvlIuWr9+Pa1ateLgwYP4+fmxYMECevXqpSNbKo+hCV+pPCQlJfHkk0/SunVrDh48yMGDB+0OSakCcSnhi8i1kk01RizXuj8spYqH7777jqioKMaNG8cjjzxCbGwszZo1szsspQrE1dEy9wBXcfmvbgHKOsq0/5nySnPnziUoKIjVq1fTpk0bu8NR6oq4mvAFMNksD8fqp6+U11i6dCnVq1cnOjqa8ePHExgYSEhIiN1hKXXFck34IjLe8a8B/isiyU7F/kAzYEvhhKZU0Tp69CiPP/44H3/8MQMHDmTWrFmULFnS7rCUchtXRssEq4ZfG2uunovOA5uB0YUQl1JFxhjD3LlzefLJJ0lMTOS1117j6ad1AFjlffL6pW0HABGZCQw1xpwpkqiUKgTHk4/z6W+fUjK5JAfOHKBySWuqhw8//JBBgwbRqlUrZsyYwY033mhzpEoVDlfntL3PXcleRPxF5BcR+dwd+1PKFYvjFlPl3So88eUT7D+zn5rjajLqk1EA9O3blw8++IDVq1drsldezaWLtiISDAwFOgEVyfJBYYyJzscxhwI7AW0cVUXiePJx+i/pnzlv7JGDR0idnsrrp1/n9oa307RaU+677z6bo1Sq8LnaS2cScCfwMbCO7Hvs5ElEKgPdgNeApwqyD6Xya9lvy/D384d0YB2MWT3G6nJwCyzZvYSm1ZraHaJSRUKMyTt3i8gJoLcx5rsrOpjIIuC/WBOnDDPGdM9mnSHAEIDIyMjGCxYsKNCxEhMTCQ8Pv4JovYevn4uE5AR2HtjJ+/99n4N7D9KseTO6DuxKydIliQyPpHJE5bx34sV8/fXhzBvORYcOHTYZY5pkV+ZqDT8Z2H8lQYhId+CYMWaTiLTPaT1jzFRgKkCTJk1M+/Y5rpqrmJgYCrqtt/Hlc2GM4dDZQ/Qb34/UiFToDb3v6M2w34cRejKUlQNW0rxyc7vDtJUvvz6y8vZz4epYOm8BT4nIlYy90xq4TUT2AguAjiIy9wr2p1SufvjhB1q0aIE5Y3jjpjcI6RNCQF2rjhMaGMr9De73+WSvfIurNfzOQBugq4jEAWnOhcaY2/LagTFmBDACwFHDH2aM6ZefYJVyRWJiIiNHjmTixIlce+21HD58mCdbPEmXGl2Yv30+kUmRWrNXPsnVGnsCsARYCRwBjme5KVUsfPPNN0RFRTFx4kQee+wxYmNjadrUuihbp0IdXu34KpUjKmuyVz7J1Rmv3NpnzRgTA8S4c59KAcyfP5+QkBDWrFlD69at7Q5HqWLF1SYdAESkCVAD+NwYkyQiYUCqMeZCoUSnlAsWL15MzZo1qV+/PuPHjycoKIjg4GC7w1Kq2HF1PPxIEfkR+AmYD0Q6isYA7xRSbErl6vDhw9x999307NmTsWPHAlCyZElN9krlwNU2/Hex2u7LYXXRvOhjoIu7g1IqN8YYZs6cSZ06dVi+fDlvvPEG06ZNszsspYo9V5t0OgGdjDEns0x8tRvQGa9UkZo1axb3338/bdq0Yfr06dSqVcvukJTyCK4m/BAuHxr5ogroBCiqCKSnp7N//36qVq3Kvffei7+/P/369cPPT6dlVspVrr5bVgODnO4bEfEH/gOscHdQSjnbuXMnbdu2pW3btiQlJVGiRAkGDBigyV6pfHK1hj8c+F5EmgIlsC7U1gVKYf2CVim3S0tL46233uLll18mPDycsWPHEhoaandYSnksV/vhx4lIPeBhIBUIxrpg+54x5nAhxqd8VHx8PJ07d2br1q307t2b8ePHExkZmfeGSqkcudwP3xhzBHihEGNRCmMMIkL58uWpV68eL774InfccYfdYSnlFVydAKVtDkUG66LtbmPMCbdFpXzS6tWr+fe//82SJUuoXLkyc+bMsTskpbyKqzX8GC5NenKxX6bz/QwRWQb0N8YkuS885QvOnDnDiBEjmDRpEtWqVePIkSNUruzbY9QrVRhc7ebQDWtawn5ATcetH7ADuNtxawC84f4QlTf78ssviYqKYvLkyTz55JNs376dJk2ynbtBKXWFXK3hvwoMNcY4d8H8U0TigTeNMY1FJB2YADzu7iCVZzuTeobFcYtJSE6gfdX2NL3m0pSCH3/8MREREaxbt44WLVrYGKVS3s/VhF8HOJjN8oOOMoDtQCV3BKW8x48HfqTLnC5kkEHqhVQC/AKon1Cf9wa8R6OGjRg3bhxBQUGUKFHC7lCV8nquNunEAc+KSOa70vH/SEcZQBWs8XaUAiDDZHDnR3dy5vwZEs8nknYqjXNzzrFh7AaefOlJACIiIjTZK1VEXK3hPwJ8BhwUkVisC7b1gAzg4kTk1YFJbo9QeazNhzeTeD7RerX8AnwNpGPNn9bD1tCU8kmu/vDqRxGphnWh9gasnjn/A+Zd7JVjjPmw0KJUHinDZFj/bAGWAdcBtwHlwPiZHLdTShWO/PzwKgl4P+tyEbnJGPOdW6NSHi89PZ0y58pQIqAEZ+udBX8gCvCDsMAw7mvg1knUlFIuKNDoUyJyjYiMEpE9WF/Ulcq0Y8cOWrduTccOHfmw24eEhYQR0igkM9m3ubYNA+oPsDtMpXyOyzV8x+iYtwEPYLXCbgMmY42poxTnz5/njTfe4NVXX6VkyZKMHz+errW7sue6PSyIXUB8cjztq7anQ9UOZJlXQSlVBPJM+CJyA/AvYACQhDXFYWesX9XG5bat8h3x8fF06tSJ7du3c8899zBu3DgqVKgAQIWwCjzeXH+eoZTdcm3SEZE1wAagNNDbGFPdGDOqKAJTnsEY6+Jr+fLladiwIcuWLWP+/PmZyV4pVXzk1YbfEvgQGGeM+b4I4lEeJCYmhiZNmrB//35EhNmzZ9Ojh/a3VKq4yivhN8Fq9lkjIr+IyP+JiP6a1sedPn2ahx56iA4dOnDq1CmOHTtmd0hKKRfkmvCNMVuMMY8CVwFjgNuB/Y7tuolImcIPURUny5cvp27dukybNo1///vfbN++ncaNG9sdllLKBS51yzTGpBhj5hhj2gO1gbeB/wOOiMiXhRifKmYWL15MmTJlWL9+PaNHj9YpB5XyIPnuh2+M2WWMeQZr7JzewHm3R6WKDWMMCxYs4JdffgFg3LhxbNq0iWbNmtkcmVIqvwr0wysAY0y6MeZTY8zt7gxIFR8HDhzg9ttv55577mHixImANdhZUFCQzZEppQqiwAlfea+MjAymTp1K3bp1+e6773jnnXeYOnWq3WEppa6Qy7+0Vb5j1qxZPPjgg3Ts2JGpU6dSo0YNu0NSSrmBJnwFWIOd7dmzh5o1a9KvXz9CQ0Pp06ePDoGglBfRJh3F9u3badmyJe3btycpKYmgoCD69u2ryV4pL6MJ34elpqbywgsv0KhRI/bu3cs777yj3SyV8mLapOOjjh07RseOHdmxYwf//Oc/GTt2LOXLl7c7LKVUIdIavhc7l3aOudvmcjjxMEt/XcqFjAuZg51VqFCBpk2b8vnnnzN37lxN9kr5gCKr4YtIFayB2CphzYU71RgzrqiO72v2nNxDyxktSUpL4sWqL/Likhcpe7gspVaXYvlny6lSpQozZ860O0ylVBEqyiadC8C/jTGbRSQC2CQi3+qY+oXj/mX3E58cT4bJ4FzSORIXJZK4OZFSV5UiISGBKlWq2B2iUqqIFVnCN8YcBg47/j8rIjuBawBN+G52Lu0ca/9aa00i/iu8Pe5tOAW0Br9b/GjYsKHdISqlbCAX23SL9KAiVYHVQJQx5kyWsiHAEIDIyMjGCxYsKNAxEhMTCQ8Pv8JIPZMxhl+O/ILBsHDqQo78eYQ7h9xJlepVCPALoH5kfbtDtJUvvzayo+fjEm84Fx06dNhkjGmSXVmRJ3wRCQe+B14zxnyS27pNmjQxGzduLNBxYmJiaN++fYG29WTGGObPn8+43ePYzGbSU9J5o/YbPPPnMwT5BzGk8RAm3DLB7jBt5auvjZzo+bjEG86FiOSY8Iu0l46IBAKLgXl5JXuVf/v376d79+7069eP6ruqExkeSUREBAEBAYQHhXNDuRt4tcOrdoeplLJJUfbSEWAGsNMYM6aojusLMjIyeP/99/nPf/5Deno6Y8eO5bHHHuOCucDSX5di9ho+6vkRN9e4GX8/f7vDVUrZpChr+K2B/kBHEdniuN1ahMf3WrNmzeKRRx6hefPmxMbGMnToUPz9/SkRUII+UX2oFF6JW6+/VZO9Uj6uKHvprAV0cBY3uXDhAnv27OH666+nX79+hIeH06tXLx3/RimVI/2lrQfaunUrLVq0oEOHDpmDnfXu3VuTvVIqV5rwPUhqairPPfccTZo0Yf/+/YwdO1YHO1NKuUwHT/MQx44do3379uzcuZMBAwYwZswYypUrZ3dYSikPojX8Ys55sLNWrVrx5ZdfMnv2bE32Sql804RfjH377bfUr1+fv/76CxFh+vTpdO3a1e6wlFIeShN+MXTy5EkGDx5Mly5dSE1N5cSJE3aHpJTyAprwi5klS5ZQp04dZs+ezYgRI9i6dSsNGjSwOyyllBfQi7bFzPLly6lUqRLLly+nUaNGdoejlPIimvBtZoxhzpw5REVF0ahRI8aNG0dQUBCBgYF2h6aU8jLapGOjffv2ccsttzBw4ECmTJkCQFhYmCZ7pVSh0IRvg4yMDN577z2ioqJYu3Yt48ePz0z4SilVWLRJxwYzZ87kscceo3PnzkydOpWqVavaHZJSygdowi8E59PP8+mvn/LjwR+pUaYG99S7h/CAcP78809q1apF//79KVmyJD179tTxb5RSRUYTvpudSjlFyxktOXDmAInnEwkNDGX4nOFUianC6eOn+f333wkLC6NXr152h6qU8jGa8N3sxZgX+fPkn5xPPw9pkLwiGdbCHxF/8NHMjwgLC7M7RKWUj9KE72YLdyy0kn0iMBM4DjQAuVVo17WdvcEppXyaJnw38xNHx6cw4DrgFqAm4IfOOKWUspV2y3Sjr7/+mvMTz1PibAlrbq/bgJrWh0Cza5pROri0zREqpXyZJnw3OHHiBIMGDaJr166UDSpL7YjahAWGEeQfRERQBFeFX8W8u+bZHaZSysdpk84VWrx4MY8++igJCQk8++yzjBo1ihIlSrD2r7VsOryJqqWr0u36bgT6669nlVL20oR/hb7++muuvvpqvvrqq8tGtWxzXRvaXNfGvsCUUioLTfj5ZIxh1qxZ1KtXjyZNmvDuu+9SokQJAgL0VCqlijdtw8+HPXv20KVLF+6//36mTZsGWIOdabJXSnkCTfguSE9PZ/z48URFRbFhwwYmTZrE5MmT7Q5LKaXyRaumLpg1axZDhw7llltuYcqUKVx77bV2h6SUUvmmCT8HaWlp7N69mxtvvJEBAwZQtmxZ7rjjDh3sTCnlsbRJJxubN2+madOmdOzYkaSkJAIDA7nzzjs12SulPJomfCfnzp3jmWeeoVmzZhw7doxJkybpYGdKKa+hTToOR48epU2bNvzxxx8MHjyY0aNHU7p0abvDUkopt/H5hJ+RkYGfnx8VK1akQ4cOTJ48mU6dOtkdllJKuZ1PN+l88cUXREVFsXfvXkSE999/X5O9Uspr+WTCT0hIoH///nTr1g0R4cyZM3aHpJRShc7nEv7ChQupU6cOCxYs4Pnnn2fz5s1ER0fbHZZSShU6n2vDX7FiBddddx0rVqygXr16doejlFJFxqsSfnpGOl/88QVf7vqSf2T8g90ndlO9THU++OADoqOjadq0Ke+++y5BQUE6/o1SyucUadYTka7AOMAfmG6MecNd+76QcYFb5t7ChoMbSDyfSM1aNan7Wl2u/+F6Yn+M5cEHH6Rp06aEhoa665BKKeVRiizhi4g/8B7QGTgA/Cwiy4wxce7Y//zt81l/YD1JaUmQAd9/8T2pH6US6xfLhPcm8MhDj7jjMEop5bGKsobfDNhljPkTQEQWALcDbkn4c7fNtZI9wC+w7LNlcD2E3x1O7a618fPzuevTSil1GTHGFM2BRHoCXY0x/3Lc7w80N8Y8lmW9IcAQgMjIyMYLFixwaf+7T+zmVOopANIvpBO/I57I6Ej8/fypVbYWYUG+O0RCYmIi4eHhdodRbOj5uJyej0u84Vx06NBhkzGmSXZlRVnDz27ksb992hhjpgJTAZo0aWLat2/v0s6Tfk9i6KKhmbX80fVHM+z3YVQMq8ih3ofw9/MvcOCeLiYmBlfPoy/Q83E5PR+XePu5KMp2jgNAFaf7lYFD7tr5rdffygONHiA4IJjQwFD8xI/SJUrz+T2f+3SyV0qpi4qyhv8zcL2IVAMOAn2Be921cxHh3a7v8lizx1i5ZyXl48tz6N+HCAkMcdchlFLKoxVZwjfGXBCRx4CvsbplfmCM2eHu49QoW4MaZWsQExOjyV4ppZwUaT98Y8wXwBdFeUyllFIW7auolFI+QhO+Ukr5CE34SinlIzThK6WUjyiyX9oWhIjEA/sKuHl5IMGN4XgyPReX0/NxOT0fl3jDubjOGFMhu4JinfCvhIhszOnnxb5Gz8Xl9HxcTs/HJd5+LrRJRymlfIQmfKWU8hHenPCn2h1AMaLn4nJ6Pi6n5+MSrz4XXtuGr5RS6nLeXMNXSinlRBO+Ukr5CK9L+CLSVUR+E5FdIvKM3fHYSUSqiMgqEdkpIjtEZKjdMdlNRPxF5BcR+dzuWOwmIqVFZJGI/Op4jbS0OyY7icj/Od4nsSLyPxEJtjsmd/OqhO80UfotQB3gHhGpY29UtroA/NsYUxtoATzq4+cDYCiw0+4giolxwFfGmBuB+vjweRGRa4AngCbGmCisIdz72huV+3lVwsdponRjzHng4kTpPskYc9gYs9nx/1msN/Q19kZlHxGpDHQDptsdi91EpCTQFpgBYIw5b4w5ZWtQ9gsAQkQkAAjFjTPyFRfelvCvAfY73T+ADyc4ZyJSFWgI/GhzKHYaCwwHMmyOozioDsQDMx1NXNNFJMzuoOxijDkIjAb+Ag4Dp40x39gblft5W8J3aaJ0XyMi4cBi4EljzBm747GDiHQHjhljNtkdSzERADQCJhtjGgJJgM9e8xKRMlitAdWAq4EwEelnb1Tu520Jv1AnSvdEIhKIleznGWM+sTseG7UGbhORvVhNfR1FZK69IdnqAHDAGHPxG98irA8AX3UTsMcYE2+MSQM+AVrZHJPbeVvCz5woXUSCsC66LLM5JtuIiGC10e40xoyxOx47GWNGGGMqG2OqYr0uVhpjvK4G5ypjzBFgv4jc4FjUCYizMSS7/QW0EJFQx/umE154EbtI57QtbEU1UboHaQ30B7aLyBbHspGOuYWVehyY56gc/QncZ3M8tjHG/Cgii4DNWL3bfsELh1nQoRWUUspHeFuTjlJKqRxowldKKR+hCV8ppXyEJnyllPIRmvCVUspHaMJXSikfoQlfFUsiMktEjIiMyrK8vWN5ebtic+b4oc7rjuG4U0QkQUR+EJF7nNaJEZGJdsapFHjZD6+U10kBhovI+8aYeLuDycEUrB+4DQVigbJAc8dfpYoVreGr4mwVsBd4LqcVsqvxi0hVx7ImWda5RUQ2icg5EVkjIpVFpJ2IbBWRRBH5XETK5TPG24D/GmM+N8bsNcZsNsZMNsa85zj2LKAd1lwExnGr6iirIyLLReSsiBxzTLpRyelxzHLENEpEjjpinCkiIU7rtBWRDY6y0yLyo4hE5fMxKB+hCV8VZxlYIzg+JCI13LC/l4AnsWrgZYCPgOeBIUB7oC7wYj73eQToKiKlcigfCqwHZgJXOW77ReQqYDXWt4JmWIN3hQPLRMT5fdkOa3KSTsDdQBfgTQDHuO2fAmsd6zTHmtQkPZ+PQfkIbdJRxZox5gsR+QF4jSufgeg5Y8waABGZAkwAGl+cJEZEZgM987nPIcA8IEFEtgPrgE+NMd864j8tIueBZMeAZTiO9TCw1RjzH6dlA4ATQBPgJ8fidOA+Y0wiECsi/wFmiMgIoARQGvjMGLPbsf6v+Yxf+RCt4StPMBzodbGJ5gpsc/r/qOPv9izLKuZnh8aY1ViTiXQEFgK1gG9E5P08Nm0MtHU0xSSKSCKXJu9x/jazzZHsL1oPBAE1jDEngFnA146moadExHl4cKUuowlfFXvGmJ+xxvR/M5vii7NXOU9+E5jDrtKcd+vYd9Zl+X5PGGPSjDFrjDFvGGO6YF1zGHKxrT4HfsByoEGW2/WAyxOsG2Puw2rKWY11PeF3Ebk5v49B+QZt0lGeYiTWeO1dsyy/2HvnKqf/GxRRTDm5OK58uOPveazhup1tBnoD+7J86GRVT0TCjDFJjvstHPu72ISDMWYrsBV4U0S+BAZiDRGu1GW0hq88gjFmF9b45EOzFO3Cagp5UURqiUgXYFTW7QtCRO4UkV9FJMd5kR197B8UkcaO3kG3Aq8Dv3FpAo29QDNHeXnHRdn3gFLARyLSXESqi8hNIjJVRCKcDhEAfCAidUWkM/AGMM0Yk+SY6OcNEWklIteJSAcgGt+eyETlQhO+8iQvY01OkclRO+6L1Y6+Fasnzkg3Ha8UcAM5NxGBVZPu7/j7KzAJWAN0NsZc7C0zGqtWHof1LeRaY8whrP77GcBXwA6sD4FUx+2i7x1lq4AlwEqsaxoAyVjXDD4GfgdmY11Azq7pSymdAEWp4srRh7+8Maa73bEo76A1fKWU8hGa8JVSykdok45SSvkIreErpZSP0ISvlFI+QhO+Ukr5CE34SinlIzThK6WUj/h/FtWieFyEyjsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "score_arr = np.array([i for i in range(10)])\n",
    "steps_arr = np.array([i for i in range(10)])\n",
    "direc_arr = np.array([\"U\",\"U\",\"U\",\"U\",\"U\",\"U\",\"U\",\"U\",\"U\",\"U\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "\n",
    "qLeft_turn_N = np.array([\"L\",\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\",\"L\"])\n",
    "qLeft_turn_N = np.array([\"R\",\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\",\"R\"])\n",
    "\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"L\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "qLeft_turn_N = np.array([\"R\"])\n",
    "\n",
    "score_step_history = np.array([steps_arr, score_arr])\n",
    "def ep_graph(steps_arr,score_arr,direction_arr):\n",
    "    # Function to map the colors as a list from the input list of x variables\n",
    "    def pltcolor(lst):\n",
    "        cols=[]\n",
    "        for l in lst:\n",
    "            if l=='U':\n",
    "                cols.append(0)\n",
    "            elif l=='L' or l=='R':\n",
    "                cols.append(1)\n",
    "            else:\n",
    "                cols.append(2)\n",
    "        return cols\n",
    "\n",
    "    colors = [\"green\", \"orange\", \"red\"]\n",
    "    colormap = matplotlib.colors.ListedColormap(colors)\n",
    "    color_indices = pltcolor(direction_arr)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(steps_arr, score_arr,'--', color='black')\n",
    "    ax.scatter(steps_arr, score_arr, c=color_indices, cmap=colormap)\n",
    "    plt.title('Agent Score Vs Action Steps', fontsize=14)\n",
    "    plt.xlabel('Num. Steps', fontsize=14)\n",
    "    plt.ylabel('Agent Score', fontsize=14)\n",
    "    \n",
    "    gre_patch = mpatches.Patch(color='green', label='Forward')\n",
    "    red_patch = mpatches.Patch(color='red', label='Back')\n",
    "    pin_patch = mpatches.Patch(color='pink', label='Right')\n",
    "    blu_patch = mpatches.Patch(color='blue', label='Left')\n",
    "    \n",
    "    plt.legend(handles=[gre_patch, red_patch, blu_patch, pin_patch])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "ep_graph(steps_arr,score_arr,direc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
