{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Beacon -env \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "North Edge - env\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN - agent \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vanilla World- \n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = ENV.sonarEnv(speed=0.5,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10003, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 500:\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        env.render(steps)\n",
    "        \n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    \n",
    "   \n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "          'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Inputs,Compass_heading] - agent\n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = ENV.sonarEnv(speed=0.5,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10003, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 500:\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        env.render(steps)\n",
    "        \n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    \n",
    "   \n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "          'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Path Construction - agent\n",
    "\"\"\"\n",
    "# Simple script to demonstrate how to use the environment as a black box.\n",
    "\n",
    "# Import the environment\n",
    "import envV2 as ENV\n",
    "#Import agent + policy model : Reinforce\n",
    "from Reinforce_keras import Agent\n",
    "# other dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment, it has a number of variables that can be initailized.\n",
    "#Here we just set the movement speed of the drone and drone size radius.\n",
    "env = ENV.sonarEnv(speed=0.5,dronesize=0.1)\n",
    "\n",
    "agent = Agent(ALPHA=0.0005, input_dims=10003, GAMMA=0.99,n_actions=4,\n",
    "             layer1_size=64, layer2_size=64)\n",
    "\n",
    "score_history = []\n",
    "steps_history = []\n",
    "stepDirct_history = [0,0,0,0]\n",
    "\n",
    "#print observation\n",
    "n_episodes = 1000\n",
    "   \n",
    "for i in range(n_episodes):\n",
    "    done=False\n",
    "    score = 0\n",
    "    steps=0\n",
    "    observation = env.reset()\n",
    "    #env.setPrefAction()\n",
    "    #prefActionEp.append(env.getPrefAction)\n",
    "    \n",
    "    while not done and steps < 500:\n",
    "        #input: get G for profomance - array len episode save G zero\n",
    "        action = agent.choose_action(observation)\n",
    "        \n",
    "        #next steps: \n",
    "        # - Reward shaping and obstical avoidance\n",
    "        # - model shaping based off input shape (Fourier series of tree echo's) \n",
    "        #   might filter input audio to make differences stand out \n",
    "        #   Qu: what differece are there in audio coming from \n",
    "        #       the front vs the back vs the sides?\n",
    "        # - Qu: What range of directions does the echo inout come from?\n",
    "        stepDirct_history[action] += 1\n",
    "            \n",
    "        #TODO: \n",
    "        # add heading \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        # Render will plot the state as a curve, and also plots a top down plot of the trees\n",
    "        env.render(steps)\n",
    "        \n",
    "    #stores the values of step for graphing\n",
    "    score_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "    \n",
    "   \n",
    "    agent.learn()\n",
    "    \n",
    "    print('episode: ', i, 'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "          'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "agent.save_model()\n",
    "\n",
    "plt.plot(score_history[:25:])\n",
    "#plt.plot(steps_history[:25:])\n",
    "plt.plot(stepDirct_history[:])\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0,0,1,1])\n",
    "#act = [0,1,2,3]\n",
    "#ax.bar(act, stepDirct_history[0:4])\n",
    "plt.show()\n",
    "# Frees some memory when finished with the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
