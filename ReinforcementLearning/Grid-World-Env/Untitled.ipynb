{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self,size, p, prefActoion):       \n",
    "        self.prefAction = prefAction\n",
    "        self.actionSpace = {'U', 'D', 'L', 'R'}\n",
    "        self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "        \n",
    "        generate_random_map(size=5, p=p)\n",
    "        setState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def onTree(state, action):\n",
    "        #if grid value at center is equal to tree at center loction set on tree to true\n",
    "        #set loction based off action\n",
    "        if action == 'U':\n",
    "            row = 0\n",
    "            col = 1\n",
    "        elif action == 'D':\n",
    "            row = -1\n",
    "            col = 1\n",
    "        elif action == 'L':\n",
    "            row = 1\n",
    "            col = 0\n",
    "        elif action == 'R':\n",
    "            row = 1\n",
    "            col = -1\n",
    "            \n",
    "        #if location matches tree \n",
    "        if state[row][col] == 'o':\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS to check that it's a valid path.\n",
    "def is_valid(res, rsize, csize):\n",
    "    #tracks a list of nodes from from (0,0) to end of graph goal\n",
    "    frontier, discovered = [], set()\n",
    "    frontier.append((0,0))\n",
    "    #loops for all connected nodes till goal node\n",
    "    while frontier:\n",
    "        #row column position\n",
    "        r, c = frontier.pop()\n",
    "        #adds to examins and adds to discovered \n",
    "        if not (r,c) in discovered:\n",
    "            discovered.add((r,c))\n",
    "            #possible x ,y directions\n",
    "            directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "            #loops through each direction\n",
    "            for x, y in directions:\n",
    "                #increasec row and colum postions exmined in x,y directions \n",
    "                r_new = r + x\n",
    "                c_new = c + y\n",
    "                #examines node and added new nods to exmin to frontier\n",
    "                if r_new < 0 or r_new >= rsize or c_new < 0 or c_new >= csize:\n",
    "                    continue\n",
    "                if res[r_new][c_new] == 'G':\n",
    "                    return True\n",
    "                if (res[r_new][c_new] !='o'):\n",
    "                    frontier.append((r_new, c_new))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['o' 'o' 'o']\n",
      " ['o' 'X' '-']\n",
      " ['-' 'o' 'o']]\n"
     ]
    }
   ],
   "source": [
    "#generates start enviroment\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    # loop tile valid generated\n",
    "    while not valid:\n",
    "        #generate random array of trres and gaps  bases of probabilty of polulation\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['-', 'o'], (size, size), p=[p, 1-p])\n",
    "        \n",
    "        #set center value to agent\n",
    "        res[2][2] = 'X'\n",
    "        \n",
    "        #set boarder edge to goal for valid travesal\n",
    "        for i in range(size):\n",
    "            res[0][i] = 'G'\n",
    "            res[-1][i] = 'G'   \n",
    "            res[i][0] = 'G'\n",
    "            res[i][-1] = 'G'\n",
    "         \n",
    "        #check if valid\n",
    "        valid = is_valid(res, size, size)\n",
    "        \n",
    "    #remove goal edge    \n",
    "    res = np.delete(res, 0, 0)\n",
    "    res = np.delete(res, -1, 0)\n",
    "    res = np.delete(res, 0, 1)\n",
    "    res = np.delete(res, -1, 1)\n",
    "        \n",
    "    return res\n",
    "\n",
    "#[\"\".join(x) for x in res],\n",
    "print(generate_random_map(size=5, p=0.5))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setState(self, state=generate_random_map(size=5, p=0.6)):\n",
    "    sState = state\n",
    "    return sState\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U\n",
      "[['-' '-' '-']\n",
      " ['o' 'X' '-']\n",
      " ['o' 'o' '-']]\n",
      "[['-' '-' 'o']\n",
      " ['-' 'X' '-']\n",
      " ['o' '-' '-']]\n",
      "\n",
      "D\n",
      "[['-' '-' '-']\n",
      " ['o' 'X' '-']\n",
      " ['o' 'o' '-']]\n",
      "None\n",
      "\n",
      "R\n",
      "[['-' '-' '-']\n",
      " ['o' 'X' '-']\n",
      " ['o' 'o' '-']]\n",
      "[['-' '-' '-']\n",
      " ['-' 'X' 'o']\n",
      " ['o' '-' '-']]\n",
      "\n",
      "L\n",
      "[['-' '-' '-']\n",
      " ['o' 'X' '-']\n",
      " ['o' 'o' '-']]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Generate new row of column of map\n",
    "def stateUpdate(state, size, action, p=0.6):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    \n",
    "    valid = False\n",
    "    \n",
    "    #loops until valid genration\n",
    "    while not valid:\n",
    "        \n",
    "        #decides col/ rows to removed based off actions taken and determins withether generatiig a row or colum in newstate\n",
    "        if action == 'U':\n",
    "            rowCol = 0\n",
    "            direction = -1\n",
    "            genRow = 1\n",
    "        elif action == 'D':\n",
    "            rowCol = 0\n",
    "            direction = 0\n",
    "            genRow = 1\n",
    "        elif action == 'L':\n",
    "            rowCol = 1\n",
    "            direction = -1\n",
    "            genRow = 0\n",
    "        elif action == 'R':\n",
    "            rowCol = 1\n",
    "            direction = 0\n",
    "            genRow = 0\n",
    "        \n",
    "        \n",
    "        #sets new space genreation based of column or row being generated\n",
    "        if genRow == 1:\n",
    "            rowSize = 2\n",
    "            colSize = size\n",
    "        else:\n",
    "            rowSize = size\n",
    "            colSize = 2\n",
    "        \n",
    "        #removes last colm/row based of action\n",
    "        newState= np.delete(stateArr, direction, rowCol)\n",
    "        \n",
    "        #repositions agent X to center space\n",
    "        if not onTree(newState, action):\n",
    "            if action == 'U':\n",
    "                temp = newState[0][1]\n",
    "                newState[0][1] = newState[1][1]\n",
    "                newState[1][1] = temp\n",
    "\n",
    "            elif action == 'D':\n",
    "                temp = newState[1][1]\n",
    "                newState[1][1] = newState[0][1]\n",
    "                newState[0][1] = temp\n",
    "\n",
    "            elif action == 'R':\n",
    "                temp = newState[1][1]\n",
    "                newState[1][1] = newState[1][0]\n",
    "                newState[1][0] = temp\n",
    "\n",
    "            elif action == 'L':\n",
    "                temp = newState[1][0]\n",
    "                newState[1][0] = newState[1][1]\n",
    "                newState[1][1] = temp\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        #generates new statespace and obstical poulation based off pprobability\n",
    "        p = min(1, p)\n",
    "        newRCArr = np.random.choice(['-', 'o'], (rowSize, colSize), p=[p, 1-p])\n",
    "        newRCStr = [\"\".join(x) for x in newRCArr]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #adds new space and sets furthest row/col as goal for path validation DFS\n",
    "        if action == 'U':\n",
    "            newState = np.concatenate((newRCArr, newState), axis=0)\n",
    "            for i in range(size):\n",
    "                newState[0][i] = 'G'\n",
    "        elif action == 'D':\n",
    "            newState = np.concatenate((newState, newRCArr), axis=0)\n",
    "            for i in range(size):\n",
    "                newState[-1][i] = 'G'\n",
    "        elif action == 'L':\n",
    "            newState = np.concatenate((newRCArr, newState), axis=1)\n",
    "            for i in range(size):\n",
    "                newState[i][0] = 'G'\n",
    "        elif action == 'R':\n",
    "            newState = np.concatenate((newState, newRCArr), axis=1)\n",
    "            for i in range(size):\n",
    "                newState[i][-1] = 'G'\n",
    "         \n",
    "        \n",
    "        #determins wither generated row/col is valid\n",
    "        if genRow == 1:\n",
    "            valid = is_valid(newState, rsize = 2+size, csize = size)\n",
    "        elif genRow == 0:\n",
    "            valid = is_valid(newState, rsize = size, csize = 2+size)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #delete goal row/col\n",
    "        if action == 'U':\n",
    "            newState = np.delete(newState, 0, 0)\n",
    "        elif action == 'D':\n",
    "            newState = np.delete(newState, -1, 0)\n",
    "        elif action == 'L':\n",
    "            newState = np.delete(newState, 0, 1)\n",
    "        elif action == 'R':\n",
    "            newState = np.delete(newState, -1, 1)       \n",
    "        \n",
    "        \n",
    "        \n",
    "    #return [\"\".join(x) for x in newState]\n",
    "    return newState\n",
    "    \n",
    "stateArr = generate_random_map(size=5, p=0.6)\n",
    "\n",
    "print(\"\\n\"+\"U\")\n",
    "print(stateArr)\n",
    "print(stateUpdate(stateArr, size=3, action='U'))\n",
    "print(\"\\n\"+\"D\")\n",
    "print(stateArr)\n",
    "print(stateUpdate(stateArr, size=3, action='D'))\n",
    "print(\"\\n\"+\"R\")\n",
    "print(stateArr)\n",
    "print(stateUpdate(stateArr, size=3, action='R'))\n",
    "print(\"\\n\"+\"L\")\n",
    "print(stateArr)\n",
    "print(stateUpdate(stateArr, size=3, action='L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "        \n",
    "        action = self.actionSpace[action]\n",
    "        \n",
    "        reward = 0 \n",
    "    \n",
    "        resultingState = None\n",
    "        \n",
    "        if not onTree(self.setState(), action):\n",
    "            \n",
    "            resultingState = stateUpdate(self.setState(), size=3, action = action, p=0.6)\n",
    "            \n",
    "            reward += 1\n",
    "            \n",
    "            if resultingState == None:\n",
    "                reward += -100\n",
    "              \n",
    "            #prefred action function\n",
    "            if self.actionSpace[action] == self.prefAction:\n",
    "                reward += -1\n",
    "            else:\n",
    "                reward += -2\n",
    "            \n",
    "            self.setState(resultingState)\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   self.onTree(resultingState), None\n",
    "                   \n",
    "        else:\n",
    "            \n",
    "            if resultingState == None:\n",
    "                reward += -100\n",
    "                resultingState = np.random.choice(['L'], (self.size, self.size))\n",
    "             \n",
    "            #prefred action function\n",
    "            if self.actionSpace[action] == self.prefAction:\n",
    "                reward += -1\n",
    "            else:\n",
    "                reward += -2\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   self.onTree(resultingState), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "        setState()\n",
    "        return self.agentPosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.setState():\n",
    "            for col in row:\n",
    "                if col == '-':\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 'X':\n",
    "                    print('X', end='\\t')\n",
    "                elif col == 'o':\n",
    "                    print('o', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionSpaceSample(self):\n",
    "        return np.random.choice(self.possibleActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxAction(Q, state, actions):\n",
    "    values = np.array([Q[state,a] for a in actions])\n",
    "    action = np.argmax(values)\n",
    "    return actions[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxAction(Q, state, actions):\n",
    "    values = np.array([Q[state,a] for a in actions])\n",
    "    action = np.argmax(values)\n",
    "    return actions[action]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # map magic squares to their connecting square\n",
    "    magicSquares = {18: 54, 63: 14}\n",
    "    env = GridWorld(9, 9, magicSquares)\n",
    "    # model hyperparameters\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 1.0\n",
    "    EPS = 1.0\n",
    "\n",
    "    Q = {}\n",
    "    for state in env.stateSpacePlus:\n",
    "        for action in env.possibleActions:\n",
    "            Q[state, action] = 0\n",
    "\n",
    "    numGames = 50000\n",
    "    totalRewards = np.zeros(numGames)\n",
    "    for i in range(numGames):\n",
    "        if i % 5000 == 0:\n",
    "            print('starting game ', i)\n",
    "        done = False\n",
    "        epRewards = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            rand = np.random.random()\n",
    "            action = maxAction(Q,observation, env.possibleActions) if rand < (1-EPS) \\\n",
    "                                                    else env.actionSpaceSample()\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            epRewards += reward\n",
    "\n",
    "            action_ = maxAction(Q, observation_, env.possibleActions)\n",
    "            Q[observation,action] = Q[observation,action] + ALPHA*(reward + \\\n",
    "                        GAMMA*Q[observation_,action_] - Q[observation,action])\n",
    "            observation = observation_\n",
    "        if EPS - 2 / numGames > 0:\n",
    "            EPS -= 2 / numGames\n",
    "        else:\n",
    "            EPS = 0\n",
    "        totalRewards[i] = epRewards\n",
    "\n",
    "    plt.plot(totalRewards)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Dense,Activation, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        input = Input(shape=(self.input_dims,))\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(input)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "        \n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        \n",
    "        \n",
    "        policy = Model(input=[input, advantages], output=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        \n",
    "        predict = Model(input=[input], output=[probs])\n",
    "        \n",
    "        return policy, predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(observation)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        actions = np.zeros([len(action_memory), self.n_actions])\n",
    "        actions[np.arange(len(action_memory)), action_memory]=1\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], actions)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    #return cost funtion\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main class\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Reinforce_keras import Agent\n",
    "from Continous_Grid_World import CGridWorld\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    prefAction = ['U', 'D', 'L', 'R']\n",
    "    \n",
    "    direction = np.random.choice(self.prefAction)\n",
    "    \n",
    "    env = CGridWorld(size = 3, p = 0.5, prefActoion = direction)\n",
    "    \n",
    "    agent = Agent(ALPHA=0.0005, input_dims=8, GAMMA=0.99,n_actions=4,\n",
    "                  layer1_size=64, layer2_size=64)\n",
    "\n",
    "    score_history = []\n",
    "    \n",
    "    #print observation\n",
    "    n_episodes = 2000\n",
    "   \n",
    "    for i in range(n_episodes):\n",
    "        done=False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            \n",
    "            action = agent.actionSpaceSample(observation)\n",
    "            \n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.store_transition(observation, action, reward)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            \n",
    "        score_history.append(score)\n",
    "        \n",
    "        agent.learn()\n",
    "        \n",
    "        print('episode ', i, 'score %.1f' % score,\n",
    "              'average_score %.1f' % np.mean(score_history[-100:]))\n",
    "    \n",
    "    \n",
    "    plt.plot(score_history)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf5ff2f21c04f4e35a0f9a3575b17121e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
