{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CGridWorld(object):\n",
    "    def __init__(self,size, p):       \n",
    "            \n",
    "            self.actionSpace = {'U':1, 'D':2, 'L':3, 'R':4}\n",
    "            self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "            \n",
    "            self.size = size\n",
    "            self.p = p\n",
    "            \n",
    "            self.prefAction = self.setPrefAction()\n",
    "            self.firstState = self.generate_random_map(size=5, p = self.p)\n",
    "            \n",
    "    def onTree(self, state, action, row=None, col=None):\n",
    "        #if grid value at center is equal to tree at center loction set on tree to true\n",
    "        \n",
    "        #set loction based off action\n",
    "        if action == 'U':\n",
    "            row = 0\n",
    "            col = 1\n",
    "        elif action == 'D':\n",
    "            row = -1\n",
    "            col = 1\n",
    "        elif action == 'L':\n",
    "            row = 1\n",
    "            col = 0\n",
    "        elif action == 'R':\n",
    "            row = 1\n",
    "            col = -1\n",
    "        \n",
    "        agentLoc = state[row][col]\n",
    "        #if location matches tree \n",
    "        if agentLoc.all() == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def setPrefAction(self, prefAction = None):\n",
    "        if prefAction is None:\n",
    "            prefAction = np.random.choice(['U', 'D', 'L', 'R'])\n",
    "        self.prefAction = prefAction\n",
    "        \n",
    "    def getPrefAction(self):\n",
    "        return self.prefAction\n",
    "            \n",
    "        \n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(self, res, rsize, csize):\n",
    "        #tracks a list of nodes from from (0,0) to end of graph goal\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        #loops for all connected nodes till goal node\n",
    "        while frontier:\n",
    "            #row column position\n",
    "            r, c = frontier.pop()\n",
    "            #adds to examins and adds to discovered \n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                #possible x ,y directions\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                #loops through each direction\n",
    "                for x, y in directions:\n",
    "                    #increasec row and colum postions exmined in x,y directions \n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    #examines node and added new nods to exmin to frontier\n",
    "                    if r_new < 0 or r_new >= rsize or c_new < 0 or c_new >= csize:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 3:\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] != 0):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "    \n",
    "   #generates start enviroment\n",
    "    def generate_random_map(self, size, p):\n",
    "        \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "        :param size: size of each side of the grid\n",
    "        :param p: probability that a tile is frozen\n",
    "        \"\"\"\n",
    "        valid = False\n",
    "        # loop tile valid generated\n",
    "        while not valid:\n",
    "            #generate random array of trres and gaps  bases of probabilty of polulation\n",
    "            self.p = min(1, p)\n",
    "            res = np.random.choice(\n",
    "                    [1, 0], (size, size), \n",
    "                                   p=[p, 1-p])\n",
    "            \n",
    "            #set center value to agent\n",
    "            res[2][2] = 2\n",
    "            \n",
    "            #set boarder edge to goal for valid travesal\n",
    "            for i in range( self.size):\n",
    "                res[0][i] = 3\n",
    "                res[-1][i] = 3   \n",
    "                res[i][0] = 3\n",
    "                res[i][-1] = 3\n",
    "             \n",
    "            #check if valid\n",
    "            valid = self.is_valid(res, size, size)\n",
    "            \n",
    "        #remove goal edge    \n",
    "        res = np.delete(res, 0, 0)\n",
    "        res = np.delete(res, -1, 0)\n",
    "        res = np.delete(res, 0, 1)\n",
    "        res = np.delete(res, -1, 1)\n",
    "            \n",
    "        return res \n",
    "    \n",
    "    def setState(self, state = None):\n",
    "        if state is None:\n",
    "            sState = self.firstState\n",
    "        else :\n",
    "            sState = state\n",
    "            \n",
    "        return sState\n",
    "    \n",
    "    #Generate new row of column of map\n",
    "    def stateUpdate(self, state, size, action, p = None):\n",
    "        \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "        :param size: size of each side of the grid\n",
    "        :param p: probability that a tile is frozen\n",
    "        \"\"\"\n",
    "        if p is None:\n",
    "            p = self.p\n",
    "        \n",
    "        \n",
    "        genRow = None\n",
    "        direction = None\n",
    "        rowCol = None\n",
    "        \n",
    "        valid = False\n",
    "        \n",
    "        #loops until valid genration\n",
    "        while not valid:\n",
    "            \n",
    "            #decides col/ rows to removed based off actions taken and determins withether generatiig a row or colum in newstate\n",
    "            if action == 'U':\n",
    "                rowCol = 0\n",
    "                direction = -1\n",
    "                genRow = 1\n",
    "            elif action == 'D':\n",
    "                rowCol = 0\n",
    "                direction = 0\n",
    "                genRow = 1\n",
    "            elif action == 'L':\n",
    "                rowCol = 1\n",
    "                direction = -1\n",
    "                genRow = 0\n",
    "            elif action == 'R':\n",
    "                rowCol = 1\n",
    "                direction = 0\n",
    "                genRow = 0\n",
    "            \n",
    "            \n",
    "            #sets new space genreation based of column or row being generated\n",
    "            if genRow == 1:\n",
    "                rowSize = 2\n",
    "                colSize = self.size\n",
    "            elif genRow == 0:\n",
    "                rowSize = self.size\n",
    "                colSize = 2\n",
    "            \n",
    "            #removes last colm/row based of action\n",
    "            newState= np.delete(state, direction, rowCol)\n",
    "            \n",
    "            #repositions agent X to center space\n",
    "            if not self.onTree(newState, action):\n",
    "                if action == 'U':\n",
    "                    temp = newState[0][1]\n",
    "                    newState[0][1] = newState[1][1]\n",
    "                    newState[1][1] = temp\n",
    "    \n",
    "                elif action == 'D':\n",
    "                    temp = newState[1][1]\n",
    "                    newState[1][1] = newState[0][1]\n",
    "                    newState[0][1] = temp\n",
    "    \n",
    "                elif action == 'R':\n",
    "                    temp = newState[1][1]\n",
    "                    newState[1][1] = newState[1][0]\n",
    "                    newState[1][0] = temp\n",
    "    \n",
    "                elif action == 'L':\n",
    "                    temp = newState[1][0]\n",
    "                    newState[1][0] = newState[1][1]\n",
    "                    newState[1][1] = temp\n",
    "            else:\n",
    "                return print(\"onTree = true\")\n",
    "                \n",
    "            \n",
    "            #generates new space or obstical poulation based off probability p\n",
    "            p = min(1, p)\n",
    "            newRCArr = np.random.choice([1, 0], (rowSize, colSize), p=[p, 1-p])\n",
    "        \n",
    "            \n",
    "            #adds new space and sets furthest row/col as goal for path validation DFS\n",
    "            if action == 'U':\n",
    "                newState = np.concatenate((newRCArr, newState), axis=0)\n",
    "                for i in range(size):\n",
    "                    newState[0][i] = 3\n",
    "            elif action == 'D':\n",
    "                newState = np.concatenate((newState, newRCArr), axis=0)\n",
    "                for i in range(size):\n",
    "                    newState[-1][i] = 3\n",
    "            elif action == 'L':\n",
    "                newState = np.concatenate((newRCArr, newState), axis=1)\n",
    "                for i in range(size):\n",
    "                    newState[i][0] = 3\n",
    "            elif action == 'R':\n",
    "                newState = np.concatenate((newState, newRCArr), axis=1)\n",
    "                for i in range(size):\n",
    "                    newState[i][-1] = 3\n",
    "             \n",
    "            \n",
    "            #determins wither generated row/col is valid\n",
    "            if genRow == 1:\n",
    "                valid = self.is_valid(newState, rsize = 2+size, csize = self.size)\n",
    "            elif genRow == 0:\n",
    "                valid = self.is_valid(newState, rsize = self.size, csize = 2+size)\n",
    "                \n",
    "            \n",
    "            \n",
    "        #delete goal row/col\n",
    "        if action == 'U':\n",
    "            newState = np.delete(newState, 0, 0)\n",
    "        elif action == 'D':\n",
    "            newState = np.delete(newState, -1, 0)\n",
    "        elif action == 'L':\n",
    "            newState = np.delete(newState, 0, 1)\n",
    "        elif action == 'R':\n",
    "            newState = np.delete(newState, -1, 1)       \n",
    "            \n",
    "            \n",
    "        return newState\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        #action = self.actionSpace[action]\n",
    "        \n",
    "        reward = 0 \n",
    "        \n",
    "        resultingState = None\n",
    "        \n",
    "        done = self.onTree(self.setState(), action)\n",
    "    \n",
    "        if not done:\n",
    "            \n",
    "            resultingState = self.stateUpdate(  self.setState(), size=3, action = action, p=0.6)\n",
    "            \n",
    "           \n",
    "            \n",
    "            reward += 15\n",
    "              \n",
    "            #prefred action function\n",
    "            if action == self.prefAction:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward += 1\n",
    "            \n",
    "            self.setState(resultingState)\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   done, None\n",
    "                   \n",
    "        else:\n",
    "            \n",
    "            if resultingState is None:\n",
    "                reward += -50\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   done, None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.setPrefAction()\n",
    "        return self.setState(self.generate_random_map(self.size + 2, self.p))\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.setState():\n",
    "            for col in row:\n",
    "                if col == 1:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 2:\n",
    "                    print('X', end='\\t')\n",
    "                elif col == 0:\n",
    "                    print('o', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Activation, Input, concatenate, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [1, 2, 3, 4]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        \n",
    "        \n",
    "        env2d = Input(shape=(self.input_dims,self.input_dims))\n",
    "        env = Flatten()(env2d)\n",
    "        \n",
    "        advantages = Input(shape=[1])\n",
    "        \n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(env)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "       \n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        \n",
    "        \n",
    "        policy = Model(input=[env2d,advantages], output=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        \n",
    "        predict = Model(input=[env2d], output=[probs])\n",
    "        \n",
    "        return policy, predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        state = observation\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(state)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(action_memory)\n",
    "        \n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        actions = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        print(actions)\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory ,self.G], actions)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    #return cost funtion\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = CGridWorld(size = 3, p = 0.5)\n",
    "    \n",
    "    agent = Agent(ALPHA=0.0005, input_dims=3, GAMMA=0.99,n_actions=4,\n",
    "                  layer1_size=64, layer2_size=64)\n",
    "\n",
    "    score_history = []\n",
    "    prefActionEp = []\n",
    "    \n",
    "    #print observation\n",
    "    n_episodes = 20\n",
    "   \n",
    "    for i in range(n_episodes):\n",
    "        done=False\n",
    "        score = 0\n",
    "        steps=0\n",
    "        observation = env.reset()\n",
    "        prefActionEp.append(env.getPrefAction)\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if action == 1:\n",
    "                strAction = 'U'\n",
    "            elif action == 2:\n",
    "                strAction = 'D'\n",
    "            elif action == 3:\n",
    "                strAction = 'L'\n",
    "            elif action == 4:\n",
    "                strAction = 'R'\n",
    "            \n",
    "            observation_, reward, done, info = env.step(strAction)\n",
    "            agent.store_transition(observation, action, reward)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            \n",
    "        \n",
    "        score_history.append(score)\n",
    "        \n",
    "        agent.learn()\n",
    "        \n",
    "        print('episode: ', i, 'prefered Action:', env.getPrefAction(),'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "              'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "    \n",
    "    \n",
    "    plt.plot(score_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf5ff2f21c04f4e35a0f9a3575b17121e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
