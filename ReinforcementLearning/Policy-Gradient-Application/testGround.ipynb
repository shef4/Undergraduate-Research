{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CGridWorld(object):\n",
    "    def __init__(self,size, p):       \n",
    "            \n",
    "            self.actionSpace = {'U':1, 'D':2, 'L':3, 'R':4}\n",
    "            self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "            \n",
    "            self.size = size\n",
    "            self.p = p\n",
    "            \n",
    "            self.prefAction = self.setPrefAction()\n",
    "            self.firstState = self.generate_random_map(size=5, p = self.p)\n",
    "            \n",
    "    def onTree(self, state, action, row=None, col=None):\n",
    "        #if grid value at center is equal to tree at center loction set on tree to true\n",
    "        \n",
    "        #set loction based off action\n",
    "        if action == 'U':\n",
    "            row = 0\n",
    "            col = 1\n",
    "        elif action == 'D':\n",
    "            row = -1\n",
    "            col = 1\n",
    "        elif action == 'L':\n",
    "            row = 1\n",
    "            col = 0\n",
    "        elif action == 'R':\n",
    "            row = 1\n",
    "            col = -1\n",
    "        \n",
    "        agentLoc = state[row][col]\n",
    "        #if location matches tree \n",
    "        if agentLoc.all() == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def setPrefAction(self, prefAction = None):\n",
    "        if prefAction is None:\n",
    "            prefAction = np.random.choice(['U', 'D', 'L', 'R'])\n",
    "        self.prefAction = prefAction\n",
    "        \n",
    "    def getPrefAction(self):\n",
    "        return self.prefAction\n",
    "            \n",
    "        \n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(self, res, rsize, csize):\n",
    "        #tracks a list of nodes from from (0,0) to end of graph goal\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        #loops for all connected nodes till goal node\n",
    "        while frontier:\n",
    "            #row column position\n",
    "            r, c = frontier.pop()\n",
    "            #adds to examins and adds to discovered \n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                #possible x ,y directions\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                #loops through each direction\n",
    "                for x, y in directions:\n",
    "                    #increasec row and colum postions exmined in x,y directions \n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    #examines node and added new nods to exmin to frontier\n",
    "                    if r_new < 0 or r_new >= rsize or c_new < 0 or c_new >= csize:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 3:\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] != 0):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "    \n",
    "   #generates start enviroment\n",
    "    def generate_random_map(self, size, p):\n",
    "        \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "        :param size: size of each side of the grid\n",
    "        :param p: probability that a tile is frozen\n",
    "        \"\"\"\n",
    "        valid = False\n",
    "        # loop tile valid generated\n",
    "        while not valid:\n",
    "            #generate random array of trres and gaps  bases of probabilty of polulation\n",
    "            self.p = min(1, p)\n",
    "            res = np.random.choice(\n",
    "                    [1, 0], (size, size), \n",
    "                                   p=[p, 1-p])\n",
    "            \n",
    "            #set center value to agent\n",
    "            res[2][2] = 2\n",
    "            \n",
    "            #set boarder edge to goal for valid travesal\n",
    "            for i in range( self.size):\n",
    "                res[0][i] = 3\n",
    "                res[-1][i] = 3   \n",
    "                res[i][0] = 3\n",
    "                res[i][-1] = 3\n",
    "             \n",
    "            #check if valid\n",
    "            valid = self.is_valid(res, size, size)\n",
    "            \n",
    "        #remove goal edge    \n",
    "        res = np.delete(res, 0, 0)\n",
    "        res = np.delete(res, -1, 0)\n",
    "        res = np.delete(res, 0, 1)\n",
    "        res = np.delete(res, -1, 1)\n",
    "            \n",
    "        return res \n",
    "    \n",
    "    def setState(self, state = None):\n",
    "        if state is None:\n",
    "            sState = self.firstState\n",
    "        else :\n",
    "            sState = state\n",
    "            \n",
    "        return sState\n",
    "    \n",
    "    #Generate new row of column of map\n",
    "    def stateUpdate(self, state, size, action, p = None):\n",
    "        \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "        :param size: size of each side of the grid\n",
    "        :param p: probability that a tile is frozen\n",
    "        \"\"\"\n",
    "        if p is None:\n",
    "            p = self.p\n",
    "        \n",
    "        \n",
    "        genRow = None\n",
    "        direction = None\n",
    "        rowCol = None\n",
    "        \n",
    "        valid = False\n",
    "        \n",
    "        #loops until valid genration\n",
    "        while not valid:\n",
    "            \n",
    "            #decides col/ rows to removed based off actions taken and determins withether generatiig a row or colum in newstate\n",
    "            if action == 'U':\n",
    "                rowCol = 0\n",
    "                direction = -1\n",
    "                genRow = 1\n",
    "            elif action == 'D':\n",
    "                rowCol = 0\n",
    "                direction = 0\n",
    "                genRow = 1\n",
    "            elif action == 'L':\n",
    "                rowCol = 1\n",
    "                direction = -1\n",
    "                genRow = 0\n",
    "            elif action == 'R':\n",
    "                rowCol = 1\n",
    "                direction = 0\n",
    "                genRow = 0\n",
    "            \n",
    "            \n",
    "            #sets new space genreation based of column or row being generated\n",
    "            if genRow == 1:\n",
    "                rowSize = 2\n",
    "                colSize = self.size\n",
    "            elif genRow == 0:\n",
    "                rowSize = self.size\n",
    "                colSize = 2\n",
    "            \n",
    "            #removes last colm/row based of action\n",
    "            newState= np.delete(state, direction, rowCol)\n",
    "            \n",
    "            #repositions agent X to center space\n",
    "            if action == 'U':\n",
    "                temp = newState[0][1]\n",
    "                newState[0][1] = newState[1][1]\n",
    "                newState[1][1] = temp\n",
    "\n",
    "            elif action == 'D':\n",
    "                temp = newState[1][1]\n",
    "                newState[1][1] = newState[0][1]\n",
    "                newState[0][1] = temp\n",
    "\n",
    "            elif action == 'R':\n",
    "                temp = newState[1][1]\n",
    "                newState[1][1] = newState[1][0]\n",
    "                newState[1][0] = temp\n",
    "\n",
    "            elif action == 'L':\n",
    "                temp = newState[1][0]\n",
    "                newState[1][0] = newState[1][1]\n",
    "                newState[1][1] = temp\n",
    "                \n",
    "            \n",
    "            #generates new space or obstical poulation based off probability p\n",
    "            p = min(1, p)\n",
    "            newRCArr = np.random.choice([1, 0], (rowSize, colSize), p=[p, 1-p])\n",
    "        \n",
    "            \n",
    "            #adds new space and sets furthest row/col as goal for path validation DFS\n",
    "            if action == 'U':\n",
    "                newState = np.concatenate((newRCArr, newState), axis=0)\n",
    "                for i in range(size):\n",
    "                    newState[0][i] = 3\n",
    "            elif action == 'D':\n",
    "                newState = np.concatenate((newState, newRCArr), axis=0)\n",
    "                for i in range(size):\n",
    "                    newState[-1][i] = 3\n",
    "            elif action == 'L':\n",
    "                newState = np.concatenate((newRCArr, newState), axis=1)\n",
    "                for i in range(size):\n",
    "                    newState[i][0] = 3\n",
    "            elif action == 'R':\n",
    "                newState = np.concatenate((newState, newRCArr), axis=1)\n",
    "                for i in range(size):\n",
    "                    newState[i][-1] = 3\n",
    "             \n",
    "            \n",
    "            #determins wither generated row/col is valid\n",
    "            if genRow == 1:\n",
    "                valid = self.is_valid(newState, rsize = 2+size, csize = self.size)\n",
    "            elif genRow == 0:\n",
    "                valid = self.is_valid(newState, rsize = self.size, csize = 2+size)\n",
    "                \n",
    "            \n",
    "            \n",
    "        #delete goal row/col\n",
    "        if action == 'U':\n",
    "            newState = np.delete(newState, 0, 0)\n",
    "        elif action == 'D':\n",
    "            newState = np.delete(newState, -1, 0)\n",
    "        elif action == 'L':\n",
    "            newState = np.delete(newState, 0, 1)\n",
    "        elif action == 'R':\n",
    "            newState = np.delete(newState, -1, 1)       \n",
    "            \n",
    "            \n",
    "        return newState\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        #action = self.actionSpace[action]\n",
    "        \n",
    "        reward = 0 \n",
    "        \n",
    "        resultingState = None\n",
    "        \n",
    "        done = self.onTree(self.setState(), action)\n",
    "    \n",
    "        if not done:\n",
    "            \n",
    "            resultingState = self.stateUpdate(  self.setState(), size=3, action = action, p=0.6)\n",
    "            \n",
    "           \n",
    "            \n",
    "            reward += 15\n",
    "              \n",
    "            #prefred action function\n",
    "            if action == self.prefAction:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward += -1\n",
    "            \n",
    "            self.setState(resultingState)\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   done, None\n",
    "                   \n",
    "        else:\n",
    "            reward += -50\n",
    "            \n",
    "            resultingState = self.stateUpdate(  self.setState(), size=3, action = action, p=0.6)\n",
    "            \n",
    "            return resultingState, reward, \\\n",
    "                   done, None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.setPrefAction()\n",
    "        return self.setState(self.generate_random_map(self.size + 2, self.p))\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.setState():\n",
    "            for col in row:\n",
    "                if col == 1:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 2:\n",
    "                    print('X', end='\\t')\n",
    "                elif col == 0:\n",
    "                    print('o', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Activation, Input, concatenate, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, ALPHA, GAMMA=0.99,n_actions=4,\n",
    "                 layer1_size=16, layer2_size=16, input_dims=128,\n",
    "                 fname='reinforce.h5'):\n",
    "        self.gamma = GAMMA\n",
    "        self.lr = ALPHA\n",
    "        self.G = 0\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network()\n",
    "        self.action_space = [1, 2, 3, 4]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    def build_policy_network(self):\n",
    "        \n",
    "        env2d = Input(shape=(self.input_dims,self.input_dims))\n",
    "        env = Flatten()(env2d)\n",
    "        \n",
    "        advantages = Input(shape=[1])\n",
    "        \n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(env)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "       \n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-8,  1-1e-8)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*advantages)\n",
    "        \n",
    "        \n",
    "        policy = Model(input=[env2d,advantages], output=[probs])\n",
    "        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)\n",
    "        \n",
    "        \n",
    "        predict = Model(input=[env2d], output=[probs])\n",
    "        \n",
    "        return policy, predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        state = observation\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(state)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    #find position around agent funtion    \n",
    "       \n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        '''label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(action_memory)\n",
    "        \n",
    "        \n",
    "        \n",
    "        onehot_encoder = OneHotEncoder(sparse=False, categories=[1, 2, 3, 4])\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        actions = onehot_encoder.fit_transform(integer_encoded)\n",
    "                    \n",
    "        \n",
    "        print(actions)'''\n",
    "        \n",
    "        G = np.zeros_like(reward_memory)\n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "        mean = np.mean(G)\n",
    "        std = np.std(G) if np.std(G) > 0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory ,self.G], action_memory)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    #return cost funtion\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sefun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sefun\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sefun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking step\n",
      "1\n",
      "taking step\n",
      "2\n",
      "taking step\n",
      "3\n",
      "taking step\n",
      "4\n",
      "taking step\n",
      "5\n",
      "WARNING:tensorflow:From C:\\Users\\sefun\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "episode:  0 prefered Action: U score -52.0: steps 4.0: average_score -52.0:\n",
      "taking step\n",
      "1\n",
      "taking step\n",
      "2\n",
      "taking step\n",
      "3\n",
      "taking step\n",
      "4\n",
      "taking step\n",
      "5\n",
      "episode:  1 prefered Action: D score -118.0: steps 4.0: average_score -85.0:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-75bf320fa84c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mstrAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'R'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrAction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2ec2a635f2f4>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m             \u001b[0mresultingState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateUpdate\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresultingState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2ec2a635f2f4>\u001b[0m in \u001b[0;36mstateUpdate\u001b[1;34m(self, state, size, action, p)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;31m#removes last colm/row based of action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mnewState\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowCol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m#repositions agent X to center space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(arr, obj, axis)\u001b[0m\n\u001b[0;32m   4384\u001b[0m         \u001b[0mslobj2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4385\u001b[0m         \u001b[0mslobj2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4386\u001b[1;33m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4387\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = CGridWorld(size = 3, p = 0.5)\n",
    "    \n",
    "    agent = Agent(ALPHA=0.0005, input_dims=3, GAMMA=0.99,n_actions=4,\n",
    "                  layer1_size=64, layer2_size=64)\n",
    "\n",
    "    score_history = []\n",
    "    prefActionEp = []\n",
    "    \n",
    "    #print observation\n",
    "    n_episodes = 10\n",
    "   \n",
    "    for i in range(n_episodes):\n",
    "        done=False\n",
    "        score = 0\n",
    "        steps=0\n",
    "        observation = env.reset()\n",
    "        prefActionEp.append(env.getPrefAction)\n",
    "        \n",
    "        while steps < 5:\n",
    "            #not done\n",
    "            action = agent.choose_action(observation)\n",
    "            \n",
    "            \n",
    "            if action == 1:\n",
    "                strAction = 'U'\n",
    "            elif action == 2:\n",
    "                strAction = 'D'\n",
    "            elif action == 3:\n",
    "                strAction = 'L'\n",
    "            elif action == 4:\n",
    "                strAction = 'R'\n",
    "            \n",
    "            observation_, reward, done, info = env.step(strAction)\n",
    "            agent.store_transition(observation, action, reward)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            print(\"taking step\")\n",
    "            print(steps)\n",
    "            \n",
    "        \n",
    "        score_history.append(score)\n",
    "        \n",
    "        agent.learn()\n",
    "        \n",
    "        print('episode: ', i, 'prefered Action:', env.getPrefAction(),'score %.1f:' % score,'steps %.1f:' % (steps-1),\n",
    "              'average_score %.1f:' % np.mean(score_history[-100:]))\n",
    "    \n",
    "    \n",
    "    plt.plot(score_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf5ff2f21c04f4e35a0f9a3575b17121e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
